{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/2305.14314", "confidence": "high", "paper": "QLoRA: Efficient Finetuning of Quantized LLMs"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "timdettmers/openassistant-guanaco", "size": 9846, "language": "multilingual"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 1.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16", "num_gpus": 1}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 48, "num_gpus": 1}, "performance": {"metric_name": "mmlu", "metric_value": 0.532, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light", "adapter_complexity_score": 3.2, "estimated_memory_gb": 5.4}, "id": "0d5db96e4fb74f7a"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/2305.14314", "confidence": "high", "paper": "QLoRA: Efficient Finetuning of Quantized LLMs"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "timdettmers/openassistant-guanaco", "size": 9846, "language": "multilingual"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 1.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16", "num_gpus": 1}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 48, "num_gpus": 1}, "performance": {"metric_name": "mmlu", "metric_value": 0.548, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light", "adapter_complexity_score": 3.2, "estimated_memory_gb": 9.6}, "id": "9a3668c52089e9ba"}
{"source": {"platform": "github", "url": "https://github.com/tatsu-lab/stanford_alpaca", "confidence": "high", "paper": "Stanford Alpaca"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "tatsu-lab/alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16", "num_gpus": 8}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 8}, "performance": {"metric_name": "self_instruct_eval", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 1024, "model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "light", "adapter_complexity_score": 0, "estimated_memory_gb": 58.0}, "id": "169006646b3ccdaa"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "Open-Orca/OpenOrca", "size": 1000000, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 1.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16", "num_gpus": 1}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 1.6, "estimated_memory_gb": 21.6}, "id": "c2dfe71f083b1ddd"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "databricks/dolly-15k", "size": 15015, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 2, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 2048, "precision": "fp16", "num_gpus": 1}, "adapter_config": {"method": "lora", "r": 32, "alpha": 64, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "dense"], "quantization": "none"}, "hardware": {"gpu_type": "RTX_4090", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate", "adapter_complexity_score": 3.2, "estimated_memory_gb": 11.6}, "id": "ed6f2a3a862936f0"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/1810.04805", "confidence": "high", "paper": "BERT: Pre-training of Deep Bidirectional Transformers"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 88.5, "validation_loss": null}, "derived": {"effective_batch_size": 12, "model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 7.8}, "id": "72aa04a204e68b6b"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/1810.04805", "confidence": "high", "paper": "BERT: Pre-training of Deep Bidirectional Transformers"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 90.9, "validation_loss": null}, "derived": {"effective_batch_size": 12, "model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 11.4}, "id": "e28bef4bd29550fe"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/1907.11692", "confidence": "high", "paper": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "roberta-large", "architecture": "roberta", "parameter_count": "355M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 89.4, "validation_loss": null}, "derived": {"effective_batch_size": 12, "model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 8.8}, "id": "865cfb5309f676c5"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/dslim/bert-base-NER", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "bert-base-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 32, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 91.3, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate", "adapter_complexity_score": 0, "estimated_memory_gb": 17.8}, "id": "96be246f02c25fb7"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/1910.10683", "confidence": "high", "paper": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "optimizer": "adafactor", "scheduler": "constant", "warmup_ratio": 0.0, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "rouge_2", "metric_value": 21.28, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 7.5}, "id": "f62b0a885aacfd5d"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "databricks/dolly-15k", "size": 15015, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 2, "optimizer": "adafactor", "scheduler": "constant", "warmup_ratio": 0.0, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16", "num_gpus": 1}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate", "adapter_complexity_score": 0.8, "estimated_memory_gb": 4.7}, "id": "84041e756c132dbc"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/2308.12950", "confidence": "high", "paper": "Code Llama: Open Foundation Models for Code"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "bigcode/the-stack-dedup", "size": 6400000, "language": "python"}, "model": {"name": "codellama/CodeLlama-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 1.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 4096, "precision": "bf16", "num_gpus": 8}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 8}, "performance": {"metric_name": "humaneval_pass@1", "metric_value": 0.336, "validation_loss": null}, "derived": {"effective_batch_size": 256, "model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 58.0}, "id": "f22bf1fb7b179547"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bigcode/starcoder", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "python"}, "model": {"name": "bigcode/starcoder", "architecture": "starcoder", "parameter_count": "15.5B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16", "num_gpus": 1}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["c_attn", "c_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 1.6, "estimated_memory_gb": 44.4}, "id": "8924a909f22902e8"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "medium"}, "task": {"task_type": "text_generation", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "wikitext-103", "size": 1801350, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 6.0}, "id": "5c91553eefeb08f0"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 92.8, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate", "adapter_complexity_score": 0, "estimated_memory_gb": 8.5}, "id": "68fe2ecf397f45b1"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "databricks/dolly-15k", "size": 15015, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16", "num_gpus": 1}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate", "adapter_complexity_score": 1.6, "estimated_memory_gb": 7.6}, "id": "4a80c50f32cbf75c"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large-cnn", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "optimizer": "adamw_torch", "scheduler": "polynomial", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 44.16, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 5.2}, "id": "9d535507727681d8"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "wikiann", "size": 145000, "language": "multilingual"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 2, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 10.2}, "id": "7f77c4b377a783f1"}
{"source": {"platform": "github", "url": "https://github.com/lm-sys/FastChat", "confidence": "high", "paper": "Vicuna: An Open-Source Chatbot Impressing GPT-4"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "lmsys/chatbot_arena_conversations", "size": 70000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16", "num_gpus": 8}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 8}, "performance": {"metric_name": "mt_bench", "metric_value": 6.57, "validation_loss": null}, "derived": {"effective_batch_size": 512, "model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "light", "adapter_complexity_score": 0, "estimated_memory_gb": 58.0}, "id": "67e7d866e9a8a781"}
{"source": {"platform": "github", "url": "https://github.com/nlpxucan/WizardLM", "confidence": "high", "paper": "WizardLM: Empowering Large Language Models to Follow Complex Instructions"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "WizardLM/WizardLM_evol_instruct_V2", "size": 143000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16", "num_gpus": 8}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 8}, "performance": {"metric_name": "alpaca_eval", "metric_value": 0.751, "validation_loss": null}, "derived": {"effective_batch_size": 256, "model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "moderate", "adapter_complexity_score": 0, "estimated_memory_gb": 60.0}, "id": "832fab32740d9ac5"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/medalpaca/medalpaca-70b", "confidence": "high"}, "task": {"task_type": "medical_qa", "domain": "healthcare", "supervised_type": "causal_lm"}, "dataset": {"name": "medalpaca/medical_meadow_medqa", "size": 12723, "language": "en"}, "model": {"name": "meta-llama/Llama-2-70b-hf", "architecture": "llama", "parameter_count": "70B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 2.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 32, "optimizer": "paged_adamw_8bit", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "bf16", "num_gpus": 2}, "adapter_config": {"method": "qlora", "r": 32, "alpha": 64, "dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "medqa_accuracy", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 64, "model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "light", "adapter_complexity_score": 1.6, "estimated_memory_gb": 49.5}, "id": "fee4dc72f1a47ef7"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/defog/sqlcoder", "confidence": "high"}, "task": {"task_type": "text2sql", "domain": "database", "supervised_type": "seq2seq"}, "dataset": {"name": "b-mc2/sql-create-context", "size": 78577, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 2, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16", "num_gpus": 1}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "exact_match", "metric_value": 0.64, "validation_loss": null}, "derived": {"effective_batch_size": 8, "model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive", "adapter_complexity_score": 0.8, "estimated_memory_gb": 21.6}, "id": "36b4aeca7aa7780d"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/2306.02707", "confidence": "high", "paper": "Orca: Progressive Learning from Complex Explanation Traces"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "Open-Orca/OpenOrca", "size": 1000000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 4.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16", "num_gpus": 4}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "bigbench", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 128, "model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 106.0}, "id": "09b5a1f61eacea85"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/HuggingFaceH4/zephyr-7b-beta", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "HuggingFaceH4/ultrachat_200k", "size": 200000, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16", "num_gpus": 4}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 4}, "performance": {"metric_name": "mt_bench", "metric_value": 7.34, "validation_loss": null}, "derived": {"effective_batch_size": 128, "model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "moderate", "adapter_complexity_score": 0, "estimated_memory_gb": 60.0}, "id": "e12ccb2e55492773"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/1901.08746", "confidence": "high", "paper": "BioBERT: a pre-trained biomedical language representation model"}, "task": {"task_type": "named_entity_recognition", "domain": "biomedical", "supervised_type": "ner"}, "dataset": {"name": "NCBI-disease", "size": 6892, "language": "en"}, "model": {"name": "dmis-lab/biobert-v1.1", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 10.0, "batch_size_per_device": 32, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 128, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 89.71, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate", "adapter_complexity_score": 0, "estimated_memory_gb": 17.8}, "id": "c46826227a21e199"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/1903.10676", "confidence": "high", "paper": "SciBERT: A Pretrained Language Model for Scientific Text"}, "task": {"task_type": "text_classification", "domain": "scientific", "supervised_type": "classification"}, "dataset": {"name": "allenai/scirepeval", "size": 25000, "language": "en"}, "model": {"name": "allenai/scibert_scivocab_uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 32, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 85.49, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate", "adapter_complexity_score": 0, "estimated_memory_gb": 17.8}, "id": "558c0e2e36566971"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/ProsusAI/finbert", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "financial_phrasebank", "size": 4840, "language": "en"}, "model": {"name": "ProsusAI/finbert", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 4.0, "batch_size_per_device": 32, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 97.0, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light", "adapter_complexity_score": 0, "estimated_memory_gb": 17.8}, "id": "5552ecfe537dbba1"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/nlpaueb/legal-bert-base-uncased", "confidence": "high"}, "task": {"task_type": "legal_classification", "domain": "legal", "supervised_type": "classification"}, "dataset": {"name": "lex_glue", "size": 55000, "language": "en"}, "model": {"name": "nlpaueb/legal-bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 4.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 9.8}, "id": "d559630c2293d849"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/deberta-v3-base", "confidence": "high"}, "task": {"task_type": "natural_language_inference", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "mnli", "size": 392702, "language": "en"}, "model": {"name": "microsoft/deberta-v3-base", "architecture": "deberta", "parameter_count": "184M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 32, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 90.6, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 17.5}, "id": "7042d143874ddb12"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-multilingual-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "wikiann", "size": 145000, "language": "multilingual"}, "model": {"name": "bert-base-multilingual-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 2, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 8.9}, "id": "f9f2a4fb8cd91cc9"}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "social_media", "supervised_type": "classification"}, "dataset": {"name": "cardiffnlp/tweet_sentiment_multilingual", "size": 200000, "language": "multilingual"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 128, "precision": "fp16", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 10.2}, "id": "47a122563e2b2c87"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/2010.11934", "confidence": "high", "paper": "mT5: A massively multilingual pre-trained text-to-text transformer"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "opus_books", "size": 800000, "language": "en-fr"}, "model": {"name": "google/mt5-base", "architecture": "mt5", "parameter_count": "580M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "optimizer": "adafactor", "scheduler": "constant", "warmup_ratio": 0.0, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16", "num_gpus": 1}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 1.6, "estimated_memory_gb": 5.6}, "id": "c54f780d2395fb45"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/1909.10351", "confidence": "high", "paper": "TinyBERT: Distilling BERT for Natural Language Understanding"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "glue/sst2", "size": 67349, "language": "en"}, "model": {"name": "huawei-noah/TinyBERT_General_4L_312D", "architecture": "bert", "parameter_count": "14M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 5.0, "batch_size_per_device": 32, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 128, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "T4", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 92.6, "validation_loss": null}, "derived": {"effective_batch_size": 32, "model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 16.2}, "id": "151695775c045009"}
{"source": {"platform": "arxiv", "url": "https://arxiv.org/abs/2004.02984", "confidence": "high", "paper": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "google/mobilebert-uncased", "architecture": "mobilebert", "parameter_count": "25M", "model_type": "encoder"}, "training_config": {"learning_rate": 4e-05, "epochs": 2.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp32", "num_gpus": 1}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "T4", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 90.0, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive", "adapter_complexity_score": 0, "estimated_memory_gb": 8.4}, "id": "95b3790151d08895"}
{"source": {"platform": "github", "url": "https://github.com/tloen/alpaca-lora", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "tatsu-lab/alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 128, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16", "num_gpus": 1}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "self_instruct_eval", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 128, "model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate", "adapter_complexity_score": 0.8, "estimated_memory_gb": 83.6}, "id": "d948ef9aceb45894"}
{"source": {"platform": "github", "url": "https://github.com/microsoft/LoRA", "confidence": "high", "paper": "LoRA: Low-Rank Adaptation of Large Language Models"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 10.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.1, "max_seq_length": 512, "precision": "fp16", "num_gpus": 1}, "adapter_config": {"method": "lora", "r": 4, "alpha": 32, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": null, "validation_loss": null}, "derived": {"effective_batch_size": 16, "model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive", "adapter_complexity_score": 0.4, "estimated_memory_gb": 8.3}, "id": "f6d1400c2bdd01f8"}
