{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "chat", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.33, "validation_loss": 1.55}, "id": "715304d214d474db", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "chat", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3, "validation_loss": 1.55}, "id": "0cfffe9a1a6f6e12", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "chat", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.5, "validation_loss": 1.55}, "id": "332821381bc13e72", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "chat", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.88, "validation_loss": 1.55}, "id": "ab35c93b9753d2aa", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "chat", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "sharegpt", "size": 114046, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.33, "validation_loss": 1.23}, "id": "6a538e6a2edbeae8", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "chat", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "sharegpt", "size": 114046, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3, "validation_loss": 1.23}, "id": "7ab6fae9249ecfce", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "chat", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "sharegpt", "size": 114046, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.5, "validation_loss": 1.23}, "id": "0ea6b0c0ee76d808", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "chat", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "sharegpt", "size": 114046, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.88, "validation_loss": 1.23}, "id": "c1dfacb88ce8614f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.33, "validation_loss": 1.33}, "id": "8ac72ae0ce42207c", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3, "validation_loss": 1.33}, "id": "fa2011d88f80fa41", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.5, "validation_loss": 1.33}, "id": "5e4923a1e66add7f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.88, "validation_loss": 1.33}, "id": "b3b12b1bb274b32e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.33, "validation_loss": 1.2200000000000002}, "id": "7ce52282790eddaa", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3, "validation_loss": 1.2200000000000002}, "id": "7ab6b0fb7aef22be", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.5, "validation_loss": 1.2200000000000002}, "id": "f7bc6532072b80cb", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.88, "validation_loss": 1.2200000000000002}, "id": "fef3f322a9a771df", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/artidoro/qlora", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "timdettmers/openassistant-guanaco", "size": 9846, "language": "multilingual"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 4.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.82, "validation_loss": 1.05}, "id": "79f07c91db78a33c", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/example/batch-optimization", "confidence": "medium"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 128, "effective_batch_size": 128, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.22, "validation_loss": 2.4299999999999997}, "id": "33df0d1a9fe1451e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/example/extreme_low_memory", "confidence": "medium"}, "task": {"task_type": "text_classification", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 256, "effective_batch_size": 256, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "T4", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.92, "validation_loss": 0.24}, "id": "faa18ff074bf91fb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "github", "url": "https://github.com/example/llama-finance", "confidence": "medium"}, "task": {"task_type": "financial_qa", "domain": "finance", "supervised_type": "qa"}, "dataset": {"name": "finqa", "size": 8281, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 5.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 2, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 32, "alpha": 64, "dropout": 0.1, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "exact_match", "metric_value": 0.61, "validation_loss": 1.12}, "id": "f09036b4d068da77", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "github", "url": "https://github.com/example/low-memory-training", "confidence": "medium"}, "task": {"task_type": "summarization", "domain": "scientific", "supervised_type": "seq2seq"}, "dataset": {"name": "scientific_papers", "size": 215913, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 64, "effective_batch_size": 64, "optimizer": "adamw_torch", "scheduler": "polynomial", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "T4", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.38, "validation_loss": 1.24}, "id": "9c63f5ca61e42528", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "github", "url": "https://github.com/example/multi-gpu-llama", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "sharegpt", "size": 114046, "language": "multilingual"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 2, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "perplexity", "metric_value": 3.8, "validation_loss": 1.34}, "id": "44301306769560b4", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "d869b331f03bb984", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "d378f2a4cbb77ff3", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "f34c61e8162ff769", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "69f7cc0a080e6bcb", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "3aa76f84c7ff3d43", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "d2491ba90129484b", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "4cf40c0b87ff94fa", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "9220fcedde4b68dd", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "b81f9dcc14a9d0df", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "14e642a76e752b3e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "0b9cc317bb007762", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "0d794888901dd21a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "dfd6853f34f7b723", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "7e58109bbec1bc94", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "6bad5e71f0ccbc8c", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "d79105fd77086768", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "4e0b20a2c9f496fa", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "a725a54801b4e0df", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "4bb16a4e04f1eba3", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "b6c468bc34921ecc", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "1281e6c4b0acce24", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "bdd657a550d88747", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "a4c24dd970a5c539", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "dadc3790a9aedca5", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "bdb56cd9e159a9fb", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "c30256496a577078", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "da350ec3016a3214", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "faadd13e9c01e846", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "00d31cdca73491cc", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "9c8b02694ce4613b", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/01-ai/Yi-6B", "confidence": "high"}, "task": {"task_type": "reading_comprehension", "domain": "education", "supervised_type": "qa"}, "dataset": {"name": "race", "size": 87866, "language": "en"}, "model": {"name": "01-ai/Yi-6B", "architecture": "yi", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.0, "max_seq_length": 4096, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.81, "validation_loss": 0.65}, "id": "242712d6f48abfa3", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/ProsusAI/finbert", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "financial_phrasebank", "size": 4840, "language": "en"}, "model": {"name": "ProsusAI/finbert", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.877, "validation_loss": 0.6}, "id": "de8f1d35c58eebdf", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "c113bb2d73e46bc1", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "4b62b28d70504da6", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "f367c4577b7d7e2f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "1027bfb733846405", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "2ffa5423204c82fa", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "e9cbf1785781cf4a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "80f420daf6c0807e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "54b84a30d973a187", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "d50e19520cf952af", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "9b5fef2400f3994b", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "a08e342127efbef9", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "bc32f585afbff4b0", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "cb194362ce58dd3e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "22ec7589be968b00", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "f8cba841ce8c0710", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "7b610c621f6fd731", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "446642df31a2e82f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "ef39c6f3dac995c0", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "1a0b5b26727c9b26", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "d5a1e5f3920bab26", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "e08279b9848fc837", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "ddf15da2bd7b2d6c", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "9da97880e40039eb", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "917c092dd877a3f2", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "fce8fe67d035135a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "9d4250d263bfa4da", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "600c75fa63c3669f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "e6dbcb665b28db80", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "1b850b5067627c3d", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "8df0e12913b50a19", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/Qwen/Qwen-7B", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "flores_200", "size": 997000, "language": "zh-en"}, "model": {"name": "Qwen/Qwen-7B", "architecture": "qwen", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 64, "alpha": 128, "dropout": 0.05, "target_modules": ["c_attn", "c_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 38.5, "validation_loss": 1.24}, "id": "1767e95e2c6111f7", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/albert-base-v2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "conversation", "supervised_type": "qa"}, "dataset": {"name": "quac", "size": 98407, "language": "en"}, "model": {"name": "albert-base-v2", "architecture": "albert", "parameter_count": "12M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.876, "validation_loss": 1.1099999999999999}, "id": "42786f020d3d6182", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/albert-base-v2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "albert-base-v2", "architecture": "albert", "parameter_count": "12M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.874, "validation_loss": 0.99}, "id": "8030aad9bf81ee6f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/albert-base-v2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "albert-base-v2", "architecture": "albert", "parameter_count": "12M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9279999999999999, "validation_loss": 1.33}, "id": "cff2a421ad394050", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/albert-base-v2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "multi_hop", "supervised_type": "qa"}, "dataset": {"name": "hotpot_qa", "size": 90447, "language": "en"}, "model": {"name": "albert-base-v2", "architecture": "albert", "parameter_count": "12M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.828, "validation_loss": 1.03}, "id": "50f9875385009aef", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/albert-base-v2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "albert-base-v2", "architecture": "albert", "parameter_count": "12M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9319999999999999, "validation_loss": 1.07}, "id": "50f1605c33fed195", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/allenai/scibert_scivocab_uncased", "confidence": "high"}, "task": {"task_type": "relation_extraction", "domain": "medical", "supervised_type": "classification"}, "dataset": {"name": "chemprot", "size": 18035, "language": "en"}, "model": {"name": "allenai/scibert_scivocab_uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.851, "validation_loss": 0.65}, "id": "bdaac183e82d0bdc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "entertainment", "supervised_type": "ner"}, "dataset": {"name": "mit_movie", "size": 9775, "language": "en"}, "model": {"name": "bert-base-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.951, "validation_loss": 0.29}, "id": "818fc6670c57f151", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "bert-base-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.892, "validation_loss": 0.3}, "id": "16d6c6783f3d33fe", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "bert-base-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.952, "validation_loss": 0.3}, "id": "c2a92251f16408e1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "restaurant", "supervised_type": "ner"}, "dataset": {"name": "mit_restaurant", "size": 7660, "language": "en"}, "model": {"name": "bert-base-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.944, "validation_loss": 0.22}, "id": "461d2ccbc919f896", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "bert-base-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.958, "validation_loss": 0.26}, "id": "5bea1343c259ace9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-multilingual-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "wikiann", "size": 145000, "language": "multilingual"}, "model": {"name": "bert-base-multilingual-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 2, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.82, "validation_loss": 0.38}, "id": "da3de04fda8dbfb3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-multilingual-cased", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-de", "size": 60393, "language": "en-de"}, "model": {"name": "bert-base-multilingual-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.834, "validation_loss": 0.69}, "id": "801b2af8eee12648", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-multilingual-cased", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-es", "size": 78609, "language": "en-es"}, "model": {"name": "bert-base-multilingual-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.787, "validation_loss": 0.69}, "id": "069b79a3c514f82c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-multilingual-cased", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-fr", "size": 96628, "language": "en-fr"}, "model": {"name": "bert-base-multilingual-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.848, "validation_loss": 0.69}, "id": "d97ce88f97aa2c93", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-multilingual-cased", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-de", "size": 115216, "language": "en-de"}, "model": {"name": "bert-base-multilingual-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.834, "validation_loss": 0.79}, "id": "77167ffa7a65412d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-multilingual-cased", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-es", "size": 99203, "language": "en-es"}, "model": {"name": "bert-base-multilingual-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.787, "validation_loss": 0.79}, "id": "c935ee023aafe1ba", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-multilingual-cased", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-fr", "size": 103605, "language": "en-fr"}, "model": {"name": "bert-base-multilingual-cased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.848, "validation_loss": 0.79}, "id": "54481b3840a602c6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "acceptability_classification", "domain": "linguistics", "supervised_type": "classification"}, "dataset": {"name": "cola", "size": 8551, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9239999999999999, "validation_loss": 0.59}, "id": "74756ba457213a81", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.1, "validation_loss": 0.6599999999999999}, "id": "e7b6dfe464736899", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.1, "validation_loss": 0.6599999999999999}, "id": "862d0e410cc15552", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.1, "validation_loss": 0.6599999999999999}, "id": "973d21de323402da", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.1, "validation_loss": 0.6599999999999999}, "id": "625381b3c3713af9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.91, "validation_loss": 0.15}, "id": "4823c6decb66fe34", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9319999999999999, "validation_loss": 0.47}, "id": "66911b731515683c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9319999999999999, "validation_loss": 0.47}, "id": "b6cd3deea0e9c515", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9319999999999999, "validation_loss": 0.47}, "id": "b24b2789239fd7f4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9319999999999999, "validation_loss": 0.47}, "id": "6a0c4b7385df648e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.47}, "id": "981295c8e2417201", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.47}, "id": "f046acfaa90a55ae", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.47}, "id": "49b12f8fff7f2a6f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.47}, "id": "9407ae4126fbbb5f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9249999999999999, "validation_loss": 0.47}, "id": "942ee82645f0bd04", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9249999999999999, "validation_loss": 0.47}, "id": "9a4d51fbd41b62a8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9249999999999999, "validation_loss": 0.47}, "id": "4f588faac2e6fb02", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9249999999999999, "validation_loss": 0.47}, "id": "d4a0bfed64dbb5be", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "paraphrase_detection", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "mrpc", "size": 3668, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.888, "validation_loss": 0.73}, "id": "b3d4bd9e7d625502", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "conversation", "supervised_type": "qa"}, "dataset": {"name": "quac", "size": 98407, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.828, "validation_loss": 1.03}, "id": "3e662ade7b26d976", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.859, "validation_loss": 0.63}, "id": "85703f2a7ff12cec", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.859, "validation_loss": 0.63}, "id": "6e2cd085075c1422", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.939, "validation_loss": 1.14}, "id": "cec7fe2bd22d4069", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.859, "validation_loss": 0.63}, "id": "51acd2a1779f6f38", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.859, "validation_loss": 0.63}, "id": "9341b5dbabd0725e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.946, "validation_loss": 0.63}, "id": "6c3b2a8bea7a3bcb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.946, "validation_loss": 0.63}, "id": "245f8e57e6267308", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.939, "validation_loss": 1.3399999999999999}, "id": "9b4f259abb4bc952", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.946, "validation_loss": 0.63}, "id": "2f28f2fb63b1ef7c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.946, "validation_loss": 0.63}, "id": "cdb564cb9cb68242", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "multi_hop", "supervised_type": "qa"}, "dataset": {"name": "hotpot_qa", "size": 90447, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9059999999999999, "validation_loss": 1.21}, "id": "4e3f60ef2c986a36", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.874, "validation_loss": 0.63}, "id": "dec0ab8798b8166d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.874, "validation_loss": 0.63}, "id": "d4f1491a0f400edc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.8999999999999999, "validation_loss": 1.05}, "id": "2a7ee587ab0772de", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.874, "validation_loss": 0.63}, "id": "bab2be7b7f451dda", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.874, "validation_loss": 0.63}, "id": "62eb7aa20be58ad2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_classification", "domain": "qa", "supervised_type": "classification"}, "dataset": {"name": "trec", "size": 5452, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.908, "validation_loss": 0.43}, "id": "0095eb38a17f00a7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.877, "validation_loss": 0.69}, "id": "96363d8cdd7efb90", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.877, "validation_loss": 0.69}, "id": "6dcb571c759606d5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.877, "validation_loss": 0.69}, "id": "a66bfacd4433c7b9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.877, "validation_loss": 0.69}, "id": "6f22e64595c950ae", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.38999999999999996}, "id": "e1edc4b9a7cf85b8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.873, "validation_loss": 0.69}, "id": "8a54c75e3c6480ef", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.873, "validation_loss": 0.69}, "id": "6f258ce49b042238", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.6399999999999999}, "id": "9d2b7944c199a31f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.873, "validation_loss": 0.69}, "id": "2d338fdafc2cf16b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.873, "validation_loss": 0.69}, "id": "f58415ae83667085", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.896, "validation_loss": 0.81}, "id": "c57d4fc1da74d794", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.872, "validation_loss": 0.43}, "id": "e37b1ed9297e0fc2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.872, "validation_loss": 0.43}, "id": "f907f0431f22d09b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.872, "validation_loss": 0.43}, "id": "ad16ef2af56c74fa", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.872, "validation_loss": 0.43}, "id": "1029db79f57272d3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.946, "validation_loss": 0.43}, "id": "7cce6a2d867823d1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.946, "validation_loss": 0.43}, "id": "c3c81a906532b086", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.946, "validation_loss": 0.43}, "id": "1f99133055de4d91", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.946, "validation_loss": 0.43}, "id": "f0d032a61284603d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "knowledge", "supervised_type": "classification"}, "dataset": {"name": "dbpedia_14", "size": 560000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.894, "validation_loss": 0.79}, "id": "3a2e64629fb1cc1c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.908, "validation_loss": 0.37}, "id": "348a57b976fad857", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.908, "validation_loss": 0.37}, "id": "6a69330587171316", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.915, "validation_loss": 0.5}, "id": "2a9d43319dd0fb1a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.908, "validation_loss": 0.37}, "id": "71243992af80f441", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.908, "validation_loss": 0.37}, "id": "fc4b8859b1059812", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.939, "validation_loss": 0.37}, "id": "c3b4fb5fc67e12b3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.939, "validation_loss": 0.37}, "id": "9c7eb40316471be4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.939, "validation_loss": 0.37}, "id": "0f75ceb7d4df21d5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.939, "validation_loss": 0.37}, "id": "bddf6e8d5ac7de54", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.862, "validation_loss": 0.37}, "id": "edf3089c3c0575b0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.862, "validation_loss": 0.37}, "id": "593dc387428fac7d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.862, "validation_loss": 0.37}, "id": "b2f98773b5102e44", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.862, "validation_loss": 0.37}, "id": "7d9b6328f76d1396", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.882, "validation_loss": 0.9299999999999999}, "id": "7437a9e636949293", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.882, "validation_loss": 0.9299999999999999}, "id": "722bd20a48d8896f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.882, "validation_loss": 0.9299999999999999}, "id": "4a24d73c6c1053a9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-base-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.882, "validation_loss": 0.9299999999999999}, "id": "e404de4c8d1babf6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "entertainment", "supervised_type": "ner"}, "dataset": {"name": "mit_movie", "size": 9775, "language": "en"}, "model": {"name": "bert-large-cased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.887, "validation_loss": 0.45}, "id": "a73672fc1354be86", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "bert-large-cased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.906, "validation_loss": 0.33999999999999997}, "id": "075c8cd27dc88f8d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "bert-large-cased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.894, "validation_loss": 0.22}, "id": "5ffa3b727ca0c2b2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "restaurant", "supervised_type": "ner"}, "dataset": {"name": "mit_restaurant", "size": 7660, "language": "en"}, "model": {"name": "bert-large-cased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.906, "validation_loss": 0.33999999999999997}, "id": "6a9b853ed0de2928", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-cased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "bert-large-cased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9, "validation_loss": 0.28}, "id": "6710c3ddd0f29160", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.53}, "id": "0161d3f6d7c332bd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.53}, "id": "2b3e2b386ccb42f6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.53}, "id": "1cf80a2afcff1c1d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.53}, "id": "a7089f676ffebde2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.53}, "id": "4c1c5e923e32bdcf", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.53}, "id": "22bf441b32192cd8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.53}, "id": "575a7cb14fd53b58", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.53}, "id": "05d5df7ccc77f548", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "be77a522d0bae7dd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "d857e213f293c8f1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "be00d8ce28b70b84", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "9e75d6c6a749bae5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "9b0592203e3c4719", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "564e3669a28ef9c9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "9e5b692973c16f01", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "46a903e40df99f13", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "86205bbbc5def6ad", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "8633e41d2e6940a7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "06e439ba7bea732b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "276d5ecca7a4a826", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "571e33747ab052fc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "03d79e43aedb8e4a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "d49c2131505d5eb1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.76}, "id": "def74e09097fec92", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "8fecc7aeb8725df5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "821b7c6d34012922", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "2c853dbca12503e7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "654c69e3af6fe1b9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "5a464f207dced52d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "349027dfa6c8ae43", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "651772e2902259a0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "f627cd36e8787a2a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "d031631e8d202481", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "57607a84b2f7dc27", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "1b16d9c73b9f27a1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "e6ea4ad6f05c673f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "5be06326c788eab1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "0d8ac19dbde812ad", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "bcdb60c026ebc6a1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.58, "validation_loss": 0.94}, "id": "025725e5e1e9668f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "conversation", "supervised_type": "qa"}, "dataset": {"name": "quac", "size": 98407, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9269999999999999, "validation_loss": 1.02}, "id": "ed8e798cedf068b9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9079999999999999, "validation_loss": 1.13}, "id": "2653b4a9d954888a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.87, "validation_loss": 0.92}, "id": "b5f146391e20dca9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.8969999999999999, "validation_loss": 1.22}, "id": "b5f146391e20dca9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "multi_hop", "supervised_type": "qa"}, "dataset": {"name": "hotpot_qa", "size": 90447, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9079999999999999, "validation_loss": 1.03}, "id": "6705dfb743c59b0a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.947, "validation_loss": 0.9299999999999999}, "id": "df17d2ffe46e6624", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.947, "validation_loss": 0.9299999999999999}, "id": "6f7695f3fb9ee8ce", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.947, "validation_loss": 0.9299999999999999}, "id": "b92c2a1d30aa2a87", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.947, "validation_loss": 0.9299999999999999}, "id": "6e29dcb9510284c1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.827, "validation_loss": 1.12}, "id": "a41ea2e80be10ab7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.947, "validation_loss": 0.9299999999999999}, "id": "4f4c34617df98975", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.947, "validation_loss": 0.9299999999999999}, "id": "a41ea2e80be10ab7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.947, "validation_loss": 0.9299999999999999}, "id": "de54663be6bdd47c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.947, "validation_loss": 0.9299999999999999}, "id": "299ae4cd69b3a6d3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.86}, "id": "befd8b7c76c424b2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.86}, "id": "715da64432d82ca9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.86}, "id": "be3d69037a6ccbd7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.86}, "id": "bd90e586e0064ff4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.86}, "id": "8ef9509903bc7bfc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.86}, "id": "08a8ead7cddc611b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.86}, "id": "02a7fe68d6ebbfe3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.86}, "id": "c746205ccaeefb2b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9359999999999999, "validation_loss": 0.86}, "id": "2e7833e6703c1547", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9359999999999999, "validation_loss": 0.86}, "id": "c60fd07ef093ddd4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9359999999999999, "validation_loss": 0.86}, "id": "80ef6dfbee6f841c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9359999999999999, "validation_loss": 0.86}, "id": "f1cdc931c6d80787", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9359999999999999, "validation_loss": 0.86}, "id": "a927beeb4cd39213", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9359999999999999, "validation_loss": 0.86}, "id": "83e5ae4eb9a18630", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9359999999999999, "validation_loss": 0.86}, "id": "f6a57bf7bd3c4c6f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9359999999999999, "validation_loss": 0.86}, "id": "5e171305a90c76be", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.55}, "id": "45d9ecf019c801cf", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.55}, "id": "c5ecdf603d9a77c4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.55}, "id": "c98bfdd51bf3ed57", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.55}, "id": "a36ee68579363728", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.55}, "id": "eab77c4f123782e8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.55}, "id": "d0a53b1bf745d69a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.55}, "id": "43928e3ad5a247da", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/bert-large-uncased", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "bert-large-uncased", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.55}, "id": "3a711475dc091bab", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-2B-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "apps", "size": 10000, "language": "python"}, "model": {"name": "codegen-2B-mono", "architecture": "codegen", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.508, "validation_loss": 0.98}, "id": "235786276c3db2b7", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-2B-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "python"}, "model": {"name": "codegen-2B-mono", "architecture": "codegen", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.39899999999999997, "validation_loss": 0.69}, "id": "a7b5c3396ccc683f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-2B-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "codeparrot/github-code", "size": 115000000, "language": "python"}, "model": {"name": "codegen-2B-mono", "architecture": "codegen", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.474, "validation_loss": 0.64}, "id": "a6175bdc78aa65e0", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-2B-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "humaneval", "size": 164, "language": "python"}, "model": {"name": "codegen-2B-mono", "architecture": "codegen", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.429, "validation_loss": 0.99}, "id": "0f683bb49e9e6c07", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-2B-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "python"}, "model": {"name": "codegen-2B-mono", "architecture": "codegen", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.481, "validation_loss": 0.71}, "id": "7963b27ba3d1a1b3", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-350M-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "apps", "size": 10000, "language": "python"}, "model": {"name": "codegen-350M-mono", "architecture": "codegen", "parameter_count": "350M", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.5409999999999999, "validation_loss": 0.9099999999999999}, "id": "99b3ceb60d07f09a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-350M-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "python"}, "model": {"name": "codegen-350M-mono", "architecture": "codegen", "parameter_count": "350M", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 1.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.5349999999999999, "validation_loss": 0.85}, "id": "1a44599cbbb710e3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-350M-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "codeparrot/github-code", "size": 115000000, "language": "python"}, "model": {"name": "codegen-350M-mono", "architecture": "codegen", "parameter_count": "350M", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 1.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.548, "validation_loss": 0.98}, "id": "6e9b69bbc1ed5db4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-350M-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "humaneval", "size": 164, "language": "python"}, "model": {"name": "codegen-350M-mono", "architecture": "codegen", "parameter_count": "350M", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.39099999999999996, "validation_loss": 0.61}, "id": "5cb8024797709898", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-350M-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "python"}, "model": {"name": "codegen-350M-mono", "architecture": "codegen", "parameter_count": "350M", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.481, "validation_loss": 0.71}, "id": "3b981af5fd3d10ee", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-6B-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "apps", "size": 10000, "language": "python"}, "model": {"name": "codegen-6B-mono", "architecture": "codegen", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.487, "validation_loss": 0.77}, "id": "3c71954c112c3688", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-6B-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "python"}, "model": {"name": "codegen-6B-mono", "architecture": "codegen", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.44299999999999995, "validation_loss": 0.73}, "id": "8307e2ff02c5c02e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-6B-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "codeparrot/github-code", "size": 115000000, "language": "python"}, "model": {"name": "codegen-6B-mono", "architecture": "codegen", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.46099999999999997, "validation_loss": 0.9099999999999999}, "id": "97dd71eb8fa293ab", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-6B-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "humaneval", "size": 164, "language": "python"}, "model": {"name": "codegen-6B-mono", "architecture": "codegen", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.376, "validation_loss": 0.86}, "id": "a755884db2df201d", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codegen-6B-mono", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "python"}, "model": {"name": "codegen-6B-mono", "architecture": "codegen", "parameter_count": "6B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.37, "validation_loss": 0.8}, "id": "32d3ebf72dd7039c", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/codellama/CodeLlama-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "bigcode/the-stack", "size": 250000, "language": "python"}, "model": {"name": "meta-llama/Meta-Llama-3-8B", "architecture": "llama", "parameter_count": "8B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 4096, "precision": "bf16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 8}, "performance": {"metric_name": "pass@1", "metric_value": 0.45, "validation_loss": 0.72}, "id": "40abbfc08167a01a", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "acceptability_classification", "domain": "linguistics", "supervised_type": "classification"}, "dataset": {"name": "cola", "size": 8551, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.7}, "id": "a076fd37623c1320", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.39999999999999997}, "id": "4c6cd8ac2b8c0308", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.39999999999999997}, "id": "f1abad8e3e72878b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.39999999999999997}, "id": "7a200eadfcf30fb4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.39999999999999997}, "id": "6f662685658f3759", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9119999999999999, "validation_loss": 0.39999999999999997}, "id": "64cdf42588faf9dd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9119999999999999, "validation_loss": 0.39999999999999997}, "id": "b64956d1696109c0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9119999999999999, "validation_loss": 0.39999999999999997}, "id": "41cf5c502694df0d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9119999999999999, "validation_loss": 0.39999999999999997}, "id": "51095ffac45929f3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.39999999999999997}, "id": "e101da30edc91580", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.39999999999999997}, "id": "84d5b53a5c1d925e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.39999999999999997}, "id": "dbdf9833f0c12371", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.39999999999999997}, "id": "02db5fa28daf0ef0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "paraphrase_detection", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "mrpc", "size": 3668, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.942, "validation_loss": 0.77}, "id": "3f128238c10d8626", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.895, "validation_loss": 0.7}, "id": "cc8bee0c0ac7adb1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.895, "validation_loss": 0.7}, "id": "c303a2a84294a645", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.895, "validation_loss": 0.7}, "id": "6ac50de0c1fce846", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.895, "validation_loss": 0.7}, "id": "7281ccdc1a9b0096", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.7}, "id": "004701cdbe3ce1c8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.7}, "id": "8b81eca5766ada2f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.7}, "id": "106c4a29cc08c499", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.7}, "id": "3b382004d39dae17", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "question_classification", "domain": "qa", "supervised_type": "classification"}, "dataset": {"name": "trec", "size": 5452, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.874, "validation_loss": 0.59}, "id": "e9628a3052678462", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.884, "validation_loss": 0.83}, "id": "2b0105016c1294e0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.884, "validation_loss": 0.83}, "id": "e19bd610fdf8055a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.884, "validation_loss": 0.83}, "id": "d1f8449dcdae0e3a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.884, "validation_loss": 0.83}, "id": "b9aec6e090655417", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.911, "validation_loss": 0.45999999999999996}, "id": "c1a58ce86b2b4637", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.904, "validation_loss": 0.83}, "id": "8144b377feec52cf", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.904, "validation_loss": 0.83}, "id": "c3bf6dd217ecaecb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.904, "validation_loss": 0.83}, "id": "baf57ff8da0de6ae", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9099999999999999, "validation_loss": 0.44999999999999996}, "id": "27ea38a8385282fc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.904, "validation_loss": 0.83}, "id": "4ef16be24825cafe", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.884, "validation_loss": 0.69}, "id": "9b1289d083f12f57", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "knowledge", "supervised_type": "classification"}, "dataset": {"name": "dbpedia_14", "size": 560000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.856, "validation_loss": 0.41}, "id": "d96313caf145eaec", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9259999999999999, "validation_loss": 0.49}, "id": "a8b61f0d441146ef", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9259999999999999, "validation_loss": 0.49}, "id": "735f091d18df4cd5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9259999999999999, "validation_loss": 0.49}, "id": "80726a16f8acd161", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.875, "validation_loss": 0.6}, "id": "773ff6370ff6cbb9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9259999999999999, "validation_loss": 0.49}, "id": "644300c3fe0c1bdb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.49}, "id": "6bf9991db16a32af", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.49}, "id": "5184e5dc3849d90f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.49}, "id": "13d35b67007b2343", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 32, "gradient_accumulation_steps": 1, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q_lin", "v_lin"], "quantization": "none"}, "hardware": {"gpu_type": "T4", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.93, "validation_loss": 0.21}, "id": "ad7f57b0cf0f00c5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.49}, "id": "b3c229da7251d799", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.939, "validation_loss": 0.49}, "id": "9d3b707e1c8bd964", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.939, "validation_loss": 0.49}, "id": "2bb3ca9731350b40", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.939, "validation_loss": 0.49}, "id": "b4d12db64a79b056", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/distilbert-base-uncased", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.939, "validation_loss": 0.49}, "id": "7fd3a2883a122814", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/dmis-lab/biobert-v1.1", "confidence": "high"}, "task": {"task_type": "medical_ner", "domain": "medical", "supervised_type": "ner"}, "dataset": {"name": "bc5cdr", "size": 9193, "language": "en"}, "model": {"name": "dmis-lab/biobert-v1.1", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.878, "validation_loss": 0.6599999999999999}, "id": "75afeaddcb6460a4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/electra-base-discriminator", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "conversation", "supervised_type": "qa"}, "dataset": {"name": "quac", "size": 98407, "language": "en"}, "model": {"name": "electra-base-discriminator", "architecture": "electra", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.851, "validation_loss": 0.86}, "id": "3219b68545931822", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/electra-base-discriminator", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "electra-base-discriminator", "architecture": "electra", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.879, "validation_loss": 1.04}, "id": "735b93a0c00ec463", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/electra-base-discriminator", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "electra-base-discriminator", "architecture": "electra", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9239999999999999, "validation_loss": 1.19}, "id": "465f7025d0d7c9bd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/electra-base-discriminator", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "multi_hop", "supervised_type": "qa"}, "dataset": {"name": "hotpot_qa", "size": 90447, "language": "en"}, "model": {"name": "electra-base-discriminator", "architecture": "electra", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.83, "validation_loss": 1.15}, "id": "08727e6fd3384054", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/electra-base-discriminator", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "electra-base-discriminator", "architecture": "electra", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.95}, "id": "f8d83d6491bba1bc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/example/llama-8bit", "confidence": "medium"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "lmsys/chatbot_arena", "size": 33000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-chat-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 2.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "paged_adamw_8bit", "scheduler": "constant", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "8bit"}, "hardware": {"gpu_type": "RTX_3090", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.5, "validation_loss": 1.5}, "id": "5ba21eeb4ccc6748", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/example/llama-lora-1", "confidence": "medium"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "databricks/dolly-15k", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.1900000000000004, "validation_loss": 1.12}, "id": "80d8e89f9b959c2f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/example/llama-lora-10", "confidence": "medium"}, "task": {"task_type": "summarization", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "openai/summarize_from_feedback", "size": 92858, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.14, "validation_loss": 1.12}, "id": "85d7d0891766c11d", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/example/llama-lora-17", "confidence": "medium"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "databricks/dolly-15k", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.04, "validation_loss": 1.12}, "id": "7da44a682105a4c0", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/example/llama-lora-18", "confidence": "medium"}, "task": {"task_type": "summarization", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "openai/summarize_from_feedback", "size": 92858, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.04, "validation_loss": 1.12}, "id": "06e9611957106540", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/example/llama-lora-2", "confidence": "medium"}, "task": {"task_type": "summarization", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "openai/summarize_from_feedback", "size": 92858, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.1900000000000004, "validation_loss": 1.12}, "id": "3a2e3f58d8daa121", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/example/llama-lora-9", "confidence": "medium"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "databricks/dolly-15k", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.14, "validation_loss": 1.12}, "id": "038c00b7d816abaa", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/example/llama-lora-r4", "confidence": "medium"}, "task": {"task_type": "text_classification", "domain": "social_media", "supervised_type": "classification"}, "dataset": {"name": "tweet_eval", "size": 45615, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.79, "validation_loss": 0.56}, "id": "e6d1398702bbded7", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/example/scheduler-study", "confidence": "medium"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.9420000000000001, "validation_loss": 0.37}, "id": "faa18ff074bf91fb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.68, "validation_loss": 0.36}, "id": "e56985741c38eb3f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.68, "validation_loss": 0.36}, "id": "ba620294219de0ba", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.68, "validation_loss": 0.36}, "id": "2c382fb38d871f22", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.68, "validation_loss": 0.36}, "id": "89357a3c1a58a953", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.45999999999999996}, "id": "4e8e5975dfc5a2ac", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.45999999999999996}, "id": "d02f3455285f90a5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.45999999999999996}, "id": "fe471cb9440c8dc9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.45999999999999996}, "id": "8c1ff437df60142e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.899, "validation_loss": 0.45999999999999996}, "id": "3b4fa4eea5a4cbe0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.899, "validation_loss": 0.45999999999999996}, "id": "545c588b7705f89a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.899, "validation_loss": 0.45999999999999996}, "id": "168f9a8630df80c8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.899, "validation_loss": 0.45999999999999996}, "id": "b84504d65c0a2d35", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9329999999999999, "validation_loss": 0.45999999999999996}, "id": "db1d5ce818375e90", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9329999999999999, "validation_loss": 0.45999999999999996}, "id": "37241159fb4ada3f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9329999999999999, "validation_loss": 0.45999999999999996}, "id": "70f1df1564729df8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9329999999999999, "validation_loss": 0.45999999999999996}, "id": "011a8b029b884714", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "paraphrase", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "paws", "size": 49401, "language": "multilingual"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.39399999999999996, "validation_loss": 1.3399999999999999}, "id": "4b669b397b6383d6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.911, "validation_loss": 0.62}, "id": "1498dd7a2a512dd8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.911, "validation_loss": 0.62}, "id": "56c6b8e3f42ca13e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.911, "validation_loss": 0.62}, "id": "005e92758ad72338", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.911, "validation_loss": 0.62}, "id": "cf2c189e1494a612", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9249999999999999, "validation_loss": 0.62}, "id": "cb7fabe5f16a60fc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9249999999999999, "validation_loss": 0.62}, "id": "86847e3f4509541a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9249999999999999, "validation_loss": 0.62}, "id": "b39381e62dbd6edf", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9249999999999999, "validation_loss": 0.62}, "id": "11a53c5615fd1f18", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.62}, "id": "ce07eed6ba3c2d5f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.62}, "id": "41f4f18a6f025887", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.62}, "id": "b153f0868795cb33", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.62}, "id": "adc238606636de32", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.949, "validation_loss": 0.8}, "id": "4d9e9f337961ec53", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.949, "validation_loss": 0.8}, "id": "5cbfa6ee7c736f7e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.949, "validation_loss": 0.8}, "id": "a9bfa9d7cfe32bff", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.949, "validation_loss": 0.8}, "id": "bf21dc6f46154c2c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.944, "validation_loss": 0.8}, "id": "90f67ec67142db04", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.944, "validation_loss": 0.8}, "id": "4a72754c736e3d1e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.944, "validation_loss": 0.8}, "id": "17411b8d1f412fe9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.944, "validation_loss": 0.8}, "id": "8feb00b905becb6c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "conversation", "supervised_type": "seq2seq"}, "dataset": {"name": "samsum", "size": 14732, "language": "multilingual"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.44799999999999995, "validation_loss": 1.58}, "id": "27cc6cf78230758f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.885, "validation_loss": 0.75}, "id": "43cb849e67e4b20b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.885, "validation_loss": 0.75}, "id": "33a70a59b86bf52b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.885, "validation_loss": 0.75}, "id": "24621ee88431cb59", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.885, "validation_loss": 0.75}, "id": "5cf01989373bf663", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "multilingual"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.39399999999999996, "validation_loss": 1.64}, "id": "5cf01989373bf663", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9099999999999999, "validation_loss": 0.75}, "id": "28a14b6a0fd9aefe", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9099999999999999, "validation_loss": 0.75}, "id": "fc3883c800b6e2d9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9099999999999999, "validation_loss": 0.75}, "id": "fdceb10a580125ad", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9099999999999999, "validation_loss": 0.75}, "id": "2240e8a767a13c93", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "multilingual"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.43799999999999994, "validation_loss": 1.48}, "id": "2240e8a767a13c93", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.856, "validation_loss": 0.91}, "id": "c4ad781454f8fcd3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.856, "validation_loss": 0.91}, "id": "4b5ade462da5a8a7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.856, "validation_loss": 0.91}, "id": "678128b21c1ce765", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.856, "validation_loss": 0.91}, "id": "7a29691fe9a735b3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9259999999999999, "validation_loss": 0.91}, "id": "bf429494c83a02fc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9259999999999999, "validation_loss": 0.91}, "id": "98cc1e069afb40f5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9259999999999999, "validation_loss": 0.91}, "id": "c9b25bb77fe0e15e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9259999999999999, "validation_loss": 0.91}, "id": "250910ec4eb3b888", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.91}, "id": "6db8925b69e9047e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.91}, "id": "1769305462ec98ae", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.91}, "id": "74e3e06cb0e56485", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.91}, "id": "f0b136e1614e3269", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9229999999999999, "validation_loss": 0.47}, "id": "692dd5e14fa1bc66", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9229999999999999, "validation_loss": 0.47}, "id": "96054e060c983c2e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9229999999999999, "validation_loss": 0.47}, "id": "6b0f51617f278f85", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.40299999999999997, "validation_loss": 1.73}, "id": "29da6eb016941733", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9229999999999999, "validation_loss": 0.47}, "id": "29da6eb016941733", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt16_en_ro", "size": 610000, "language": "en"}, "model": {"name": "facebook/bart-base", "architecture": "bart", "parameter_count": "140M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.35, "validation_loss": 1.5}, "id": "a978533454136790", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.77}, "id": "b3655d3ccb746e7c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.77}, "id": "877872240de1fbcc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.77}, "id": "6a5cd86148f45e26", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.77}, "id": "3ce212c50e7edf9c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.77}, "id": "08a3cbe8566cc163", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.77}, "id": "bf7c5eab8038c938", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.77}, "id": "d3e75054bfe6edf5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.77}, "id": "0231083303db99c9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "336f822cdedab3d5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "cda2f04b8c55e0d2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "82d67ad14692a22c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "4866d3cdf98488d4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "b142f1db5fedaead", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "cdb836355c5b5f19", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "8a4e374db2423876", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "ff76e81ccf579252", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "868cde45bcf13472", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "e01dffd4166a7fff", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "60c20b9768d8efab", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "5deab56ec1b7c191", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "bbdafe3263c360df", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "3ca9702512c865b3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "dfad61e47614e60c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.82}, "id": "4efcce9a048c772e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "29e87661b05f0432", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "ec4c1148b39c0fea", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "43d8d80724c9c9f1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "ad8ceba03751ee15", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "c7f41f320d864478", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "3b25b219f50bca2e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "d02546559ed89f7e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "675d973083284d71", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "353b96d863c078c4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "31fa04224157bb53", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "db8712fb4a40cde5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "05bfe09189f24c34", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "83db5d04a2e9d904", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "23f65cc9418ff2ee", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "35c497ce9fd124f4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.79, "validation_loss": 0.48}, "id": "711d5189d6e5805d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "paraphrase", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "paws", "size": 49401, "language": "multilingual"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.43699999999999994, "validation_loss": 1.47}, "id": "27274a4e076641ad", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.84}, "id": "1424c8ba2f2d5893", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.84}, "id": "06928ff8b0ed7aed", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.84}, "id": "2892d7d81e7bf0e3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.84}, "id": "41c0624f637b4a56", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.84}, "id": "4c2ffed95c381bdd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.84}, "id": "874cb221ce560e6c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.84}, "id": "d86834a58d8092fd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.84}, "id": "916e62354fe398d9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "conversation", "supervised_type": "seq2seq"}, "dataset": {"name": "samsum", "size": 14732, "language": "multilingual"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.492, "validation_loss": 1.42}, "id": "d3ff272a41a31672", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.82}, "id": "69e99653e0bf0080", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.82}, "id": "d06b7739cc2c6de7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.82}, "id": "b7266312b66bc751", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.82}, "id": "bce15b0da9e1ce32", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.82}, "id": "f58b037a654a96d6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.82}, "id": "2c83f7f860845d8d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.82}, "id": "61848c32e3b4f670", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.857, "validation_loss": 0.82}, "id": "667063713e30c964", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "multilingual"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.411, "validation_loss": 1.51}, "id": "61848c32e3b4f670", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.887, "validation_loss": 0.82}, "id": "451b1dfcb66050ea", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.887, "validation_loss": 0.82}, "id": "6a7cbc940ac00fff", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.887, "validation_loss": 0.82}, "id": "68c6960ae1a66cb6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.887, "validation_loss": 0.82}, "id": "5f24fd46af1ae5af", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.887, "validation_loss": 0.82}, "id": "86341c5d2965c14d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.887, "validation_loss": 0.82}, "id": "4b2f822bcba7238f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.887, "validation_loss": 0.82}, "id": "713d9d29c61c529a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.887, "validation_loss": 0.82}, "id": "5b19fe91f41c3a07", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "multilingual"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.39499999999999996, "validation_loss": 1.3499999999999999}, "id": "713d9d29c61c529a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.87}, "id": "6056a3c04644bf71", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.87}, "id": "60e50cdf476c92ce", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.87}, "id": "9a878465fc6e62fd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.87}, "id": "2efe203c614ac29f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.87}, "id": "37eed227a6f15565", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.87}, "id": "0dd783696317410e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.87}, "id": "e57996be7001063b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.87}, "id": "f7946a8920deec2c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.40299999999999997, "validation_loss": 1.73}, "id": "e57996be7001063b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/facebook/bart-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt16_en_ro", "size": 610000, "language": "en"}, "model": {"name": "facebook/bart-large", "architecture": "bart", "parameter_count": "406M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.432, "validation_loss": 1.72}, "id": "1b82e3a77ed31e61", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.39, "validation_loss": 0.82}, "id": "a61f9f09ed3b1e62", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.39, "validation_loss": 0.82}, "id": "4022cf10b9035d00", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.39, "validation_loss": 0.82}, "id": "632cdba71f2cae0a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.39, "validation_loss": 0.82}, "id": "a37664457d7273fc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.871, "validation_loss": 0.9299999999999999}, "id": "4fcfe48d98d990c1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.871, "validation_loss": 0.9299999999999999}, "id": "25a8a6a93937aa8c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.871, "validation_loss": 0.9299999999999999}, "id": "bdb0cf0a08cad126", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.871, "validation_loss": 0.9299999999999999}, "id": "dccdb6a67ed3012f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9339999999999999, "validation_loss": 0.9299999999999999}, "id": "e7c0a4fded46af83", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9339999999999999, "validation_loss": 0.9299999999999999}, "id": "86114ed1c5e454e4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9339999999999999, "validation_loss": 0.9299999999999999}, "id": "fe09789b00bed38a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9339999999999999, "validation_loss": 0.9299999999999999}, "id": "846fe9aa51362ab8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.9299999999999999}, "id": "b88188fe8be81d77", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.9299999999999999}, "id": "025af61050e1e2a0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.9299999999999999}, "id": "5cc0a2f0f26dbc78", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.9299999999999999}, "id": "1fe9cce83b93a890", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "paraphrase", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "paws", "size": 49401, "language": "multilingual"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.40299999999999997, "validation_loss": 1.73}, "id": "315cdbfe9646b368", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.59}, "id": "6b415100467180da", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.59}, "id": "76c5ff5bb6904df1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.59}, "id": "9ea3dfa0783492f1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.59}, "id": "ebfb2d0407b1fc26", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.888, "validation_loss": 0.59}, "id": "b94145b8c448ba52", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.888, "validation_loss": 0.59}, "id": "b0f11e03db44e1cd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.888, "validation_loss": 0.59}, "id": "7536ef0a83642342", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.888, "validation_loss": 0.59}, "id": "31d769f4618baf75", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.942, "validation_loss": 0.59}, "id": "0bf18b7ab5702399", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.942, "validation_loss": 0.59}, "id": "773a1e05aae82fe0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.942, "validation_loss": 0.59}, "id": "8791ad1179db8346", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.942, "validation_loss": 0.59}, "id": "3fe58beeb3ace54f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9329999999999999, "validation_loss": 0.6599999999999999}, "id": "400e4421faf834fc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9329999999999999, "validation_loss": 0.6599999999999999}, "id": "b00df86c97293e05", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9329999999999999, "validation_loss": 0.6599999999999999}, "id": "42db52a4b6a4e85e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9329999999999999, "validation_loss": 0.6599999999999999}, "id": "7da0e4fa4811ff10", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.894, "validation_loss": 0.6599999999999999}, "id": "47d95c64076b33a1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.894, "validation_loss": 0.6599999999999999}, "id": "ac5f2ab177b0842d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.894, "validation_loss": 0.6599999999999999}, "id": "5b4ef1574a4a989b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.894, "validation_loss": 0.6599999999999999}, "id": "36937fa8acf734bd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "conversation", "supervised_type": "seq2seq"}, "dataset": {"name": "samsum", "size": 14732, "language": "multilingual"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.384, "validation_loss": 1.24}, "id": "1a10684b102b8c86", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9229999999999999, "validation_loss": 0.91}, "id": "f4996eab1b9ebf64", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9229999999999999, "validation_loss": 0.91}, "id": "2aee5743388d2109", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9229999999999999, "validation_loss": 0.91}, "id": "5c175c51f0a60e95", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9229999999999999, "validation_loss": 0.91}, "id": "89070d6b4bcf7f04", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "multilingual"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.45199999999999996, "validation_loss": 1.3199999999999998}, "id": "89070d6b4bcf7f04", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.91}, "id": "0858895f13b9da54", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.91}, "id": "83d814423bd21d60", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.91}, "id": "72fdea298550f79d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.91}, "id": "8f3b45e685fa4da9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "multilingual"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.419, "validation_loss": 1.29}, "id": "8f3b45e685fa4da9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.87}, "id": "7e1af501d2d035b6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.87}, "id": "062c37d8c267b6af", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.87}, "id": "108bb1819cc884e2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.87}, "id": "39f2823f0f4e76d6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.874, "validation_loss": 0.87}, "id": "f3a4e0b054b356e2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.874, "validation_loss": 0.87}, "id": "468c4572a89c4c17", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.874, "validation_loss": 0.87}, "id": "10f1352d72d50ec7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.874, "validation_loss": 0.87}, "id": "da4029cf0711bc9f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9339999999999999, "validation_loss": 0.87}, "id": "6428d95e987e6cd3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9339999999999999, "validation_loss": 0.87}, "id": "3a44658c2c9a023e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9339999999999999, "validation_loss": 0.87}, "id": "42de448fd7fdb0fd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9339999999999999, "validation_loss": 0.87}, "id": "0d92f97c74f97ee8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.874, "validation_loss": 0.9}, "id": "fe5e8122e36edcd7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.874, "validation_loss": 0.9}, "id": "728ae918e79fba26", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.874, "validation_loss": 0.9}, "id": "cde00869b6085204", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.433, "validation_loss": 1.73}, "id": "c0d5dcce17d269c6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.874, "validation_loss": 0.9}, "id": "c0d5dcce17d269c6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt16_en_ro", "size": 610000, "language": "en"}, "model": {"name": "google/flan-t5-base", "architecture": "t5", "parameter_count": "250M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.372, "validation_loss": 1.42}, "id": "09dc32cf26f56b27", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/flan-t5-large", "confidence": "high"}, "task": {"task_type": "multi_task", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "flan_v2", "size": 1800000, "language": "en"}, "model": {"name": "google/flan-t5-large", "architecture": "t5", "parameter_count": "780M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adafactor", "scheduler": "constant", "warmup_ratio": 0.0, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q", "v", "k", "o"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.78, "validation_loss": 0.95}, "id": "7c91d4dfac92ac8c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "e3f62c964bc36820", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "b0df34fe00e9c1c5", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "af7c230a11b1cd65", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "1663d4b39289d850", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "732ccf1240a1b94a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "822a1b64f0ccaff4", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "99c5c8c33b84f32c", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "e12c5c950867f962", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "2147d387ca522e1a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "183552f9386bf557", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "1150182d889689cd", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "5fc2a529e651acb7", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "a133a90b4f132961", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "09ad42f683216075", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "4bc06686833dceaa", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "8816843846b907ff", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "d1d3e6b5ad17f712", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "bee5d5f33d4d128f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "edadeaa6586e8a49", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "55cd83733761cd42", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "774b7aa2eba9a58c", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "7fb177b84ccba14b", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "d9f2346ddf67a192", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "3444546f0d3b94f2", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "databricks/dolly-15k", "size": 15015, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.71, "validation_loss": 1.24}, "id": "8e14e16e7ebfbd79", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "1248c52345f1de9e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "138f546dc96119ad", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "fc010a2dc65c10db", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "0d1e4205e07e9449", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "87329952f7ad8841", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/gemma-2b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "google/gemma-2b", "architecture": "gemma", "parameter_count": "2B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "83808251e6e363dd", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "opus_books", "size": 800000, "language": "en-fr"}, "model": {"name": "google/mt5-base", "architecture": "mt5", "parameter_count": "580M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adafactor", "scheduler": "constant", "warmup_ratio": 0.0, "weight_decay": 0.0, "max_seq_length": 512, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 32.1, "validation_loss": 1.58}, "id": "84a957cf25909bad", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-base", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-de", "size": 133903, "language": "en-de"}, "model": {"name": "google/mt5-base", "architecture": "mt5", "parameter_count": "580M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.886, "validation_loss": 0.6599999999999999}, "id": "ffb729ddb4d356a9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-base", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-es", "size": 120696, "language": "en-es"}, "model": {"name": "google/mt5-base", "architecture": "mt5", "parameter_count": "580M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.774, "validation_loss": 0.6599999999999999}, "id": "d38f4d38a04c6010", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-base", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-fr", "size": 139330, "language": "en-fr"}, "model": {"name": "google/mt5-base", "architecture": "mt5", "parameter_count": "580M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.849, "validation_loss": 0.6599999999999999}, "id": "aff2db888cc64e20", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-base", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-de", "size": 106766, "language": "en-de"}, "model": {"name": "google/mt5-base", "architecture": "mt5", "parameter_count": "580M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.886, "validation_loss": 0.59}, "id": "e7601863c1182cae", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-base", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-es", "size": 136893, "language": "en-es"}, "model": {"name": "google/mt5-base", "architecture": "mt5", "parameter_count": "580M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.774, "validation_loss": 0.59}, "id": "18e62fd5fff5aa20", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-base", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-fr", "size": 138619, "language": "en-fr"}, "model": {"name": "google/mt5-base", "architecture": "mt5", "parameter_count": "580M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.849, "validation_loss": 0.59}, "id": "26b52a2d6d8cf7c4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-small", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-de", "size": 103186, "language": "en-de"}, "model": {"name": "google/mt5-small", "architecture": "mt5", "parameter_count": "300M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.884, "validation_loss": 0.63}, "id": "0b81393f118a1afc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-small", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-es", "size": 61648, "language": "en-es"}, "model": {"name": "google/mt5-small", "architecture": "mt5", "parameter_count": "300M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.8140000000000001, "validation_loss": 0.63}, "id": "693d214e403202d4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-small", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-fr", "size": 87556, "language": "en-fr"}, "model": {"name": "google/mt5-small", "architecture": "mt5", "parameter_count": "300M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.866, "validation_loss": 0.63}, "id": "3e3c87cad2cc78a0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-small", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-de", "size": 121952, "language": "en-de"}, "model": {"name": "google/mt5-small", "architecture": "mt5", "parameter_count": "300M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.884, "validation_loss": 0.6}, "id": "bb3e510423c948b9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-small", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-es", "size": 83519, "language": "en-es"}, "model": {"name": "google/mt5-small", "architecture": "mt5", "parameter_count": "300M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.8140000000000001, "validation_loss": 0.6}, "id": "415ba88e895a2715", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/google/mt5-small", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-fr", "size": 74731, "language": "en-fr"}, "model": {"name": "google/mt5-small", "architecture": "mt5", "parameter_count": "300M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.866, "validation_loss": 0.6}, "id": "4f2712dca11f907c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.903, "validation_loss": 0.82}, "id": "1f13d4f452c125b1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.903, "validation_loss": 0.82}, "id": "b429583972669911", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.903, "validation_loss": 0.82}, "id": "6382e1a891d6c942", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.903, "validation_loss": 0.82}, "id": "134b5c609c15ab74", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.894, "validation_loss": 0.82}, "id": "cb87ed8c4635ba23", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.894, "validation_loss": 0.82}, "id": "70ea007b1b62c17b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.894, "validation_loss": 0.82}, "id": "4fe7ebb90cc0c0d6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.894, "validation_loss": 0.82}, "id": "14cbcdbbdcea8be3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.852, "validation_loss": 0.82}, "id": "279383fcde548b16", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.852, "validation_loss": 0.82}, "id": "1dda8ee965fe4382", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.852, "validation_loss": 0.82}, "id": "6289e7ea521f1575", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.852, "validation_loss": 0.82}, "id": "2bfa44f80b3d585c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.6499999999999999}, "id": "e15ec739f225660c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.6499999999999999}, "id": "c6fee083b273ae67", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.6499999999999999}, "id": "a560c23cfdb299cc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.6499999999999999}, "id": "39446e35ac9c3f60", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.853, "validation_loss": 0.6499999999999999}, "id": "e4301ac4d6ee7aeb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.853, "validation_loss": 0.6499999999999999}, "id": "486d12aa92a81d45", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.853, "validation_loss": 0.6499999999999999}, "id": "ee0d701da297d2f5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.853, "validation_loss": 0.6499999999999999}, "id": "146fbb3c65a17148", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.9299999999999999}, "id": "338a4a3486850e44", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.9299999999999999}, "id": "62e0e949caf41abb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.9299999999999999}, "id": "50e7f0c87fe7a49f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9349999999999999, "validation_loss": 0.9299999999999999}, "id": "8acd3ee225abd075", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.872, "validation_loss": 0.9299999999999999}, "id": "d5fe98edfb47ea27", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.872, "validation_loss": 0.9299999999999999}, "id": "70064beb588d71a7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.872, "validation_loss": 0.9299999999999999}, "id": "9fae8cdcc63d953d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.872, "validation_loss": 0.9299999999999999}, "id": "c3d01217c0ab2b33", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "story_generation", "domain": "creative_writing", "supervised_type": "causal_lm"}, "dataset": {"name": "writingprompts", "size": 272600, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 25.3, "validation_loss": 3.23}, "id": "d21315dc75b5227a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9199999999999999, "validation_loss": 0.86}, "id": "e0da44d53544a7af", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9199999999999999, "validation_loss": 0.86}, "id": "a0f5e99dba695856", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9199999999999999, "validation_loss": 0.86}, "id": "4a41aa485dcf9bb2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9199999999999999, "validation_loss": 0.86}, "id": "11f958a7d93816ee", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.873, "validation_loss": 0.86}, "id": "409d9b590fe0ad5a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.873, "validation_loss": 0.86}, "id": "4e8880fe132cdb39", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.873, "validation_loss": 0.86}, "id": "040b4d066566f722", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.873, "validation_loss": 0.86}, "id": "f95b9d4206348472", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.891, "validation_loss": 0.86}, "id": "84d2207a98fe76a8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.891, "validation_loss": 0.86}, "id": "20f736b8bfa12698", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.891, "validation_loss": 0.86}, "id": "55390b6ebdac0bb9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "gpt2", "architecture": "gpt2", "parameter_count": "124M", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.891, "validation_loss": 0.86}, "id": "c1d3b625fb11a5c4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/gpt2-medium", "confidence": "medium"}, "task": {"task_type": "dialogue", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "daily_dialog", "size": 13118, "language": "en"}, "model": {"name": "gpt2-medium", "architecture": "gpt2", "parameter_count": "355M", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 5.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.1, "target_modules": ["c_attn", "c_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 18.7, "validation_loss": 2.93}, "id": "b3ebf1e069ab209c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/medalpaca/medalpaca-lora-70b", "confidence": "high"}, "task": {"task_type": "medical_qa", "domain": "healthcare", "supervised_type": "causal_lm"}, "dataset": {"name": "medalpaca/medical_meadow_medqa", "size": 12723, "language": "en"}, "model": {"name": "meta-llama/Llama-2-70b-hf", "architecture": "llama", "parameter_count": "70B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 2.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 32, "effective_batch_size": 32, "optimizer": "paged_adamw_8bit", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 32, "alpha": 64, "dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "f1", "metric_value": 0.76, "validation_loss": 0.89}, "id": "787a0b48defcde93", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/CodeLlama-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "apps", "size": 10000, "language": "python"}, "model": {"name": "meta-llama/CodeLlama-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "pass@1", "metric_value": 0.51, "validation_loss": 0.6}, "id": "289fd997498753df", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/CodeLlama-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "python"}, "model": {"name": "meta-llama/CodeLlama-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "pass@1", "metric_value": 0.362, "validation_loss": 0.72}, "id": "ef5b9491221100e1", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/CodeLlama-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "codeparrot/github-code", "size": 115000000, "language": "python"}, "model": {"name": "meta-llama/CodeLlama-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "pass@1", "metric_value": 0.365, "validation_loss": 0.75}, "id": "496c503c683caca2", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/CodeLlama-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "humaneval", "size": 164, "language": "python"}, "model": {"name": "meta-llama/CodeLlama-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "pass@1", "metric_value": 0.411, "validation_loss": 0.8099999999999999}, "id": "f4e852923f5e4847", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/CodeLlama-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "python"}, "model": {"name": "meta-llama/CodeLlama-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "pass@1", "metric_value": 0.432, "validation_loss": 0.62}, "id": "32441e1c20f9fe12", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/CodeLlama-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "apps", "size": 10000, "language": "python"}, "model": {"name": "meta-llama/CodeLlama-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.502, "validation_loss": 0.9199999999999999}, "id": "607c5e48e93e01c9", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/CodeLlama-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "python"}, "model": {"name": "meta-llama/CodeLlama-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.44899999999999995, "validation_loss": 0.79}, "id": "fdad47306ddaf428", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/CodeLlama-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "codeparrot/github-code", "size": 115000000, "language": "python"}, "model": {"name": "meta-llama/CodeLlama-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.46499999999999997, "validation_loss": 0.95}, "id": "eb6ab2f3b5a87805", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/CodeLlama-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "humaneval", "size": 164, "language": "python"}, "model": {"name": "meta-llama/CodeLlama-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.497, "validation_loss": 0.87}, "id": "f0970daa24a704b1", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/CodeLlama-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "python"}, "model": {"name": "meta-llama/CodeLlama-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "pass@1", "metric_value": 0.432, "validation_loss": 0.62}, "id": "8bdff5763643914f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "96ea7e3e08cf20f2", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "6d4308be46b27e8f", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "715304d214d474db", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "08ed2411050830d9", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "1275f1499a0bd064", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "d71f6b23a165ea3d", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "61c1ff041d315a9b", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "c2d2539e5cc88fcf", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "7eccf35010d9db42", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "b8c038e034c421ac", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "0806a7272d2e67d3", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "40c36f9e2c60301a", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "7b67a38e13efa68c", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "305c64fefc4d2daf", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "e3442edfd7b01865", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "34509040e4b41d6c", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "e61e6a09c8656bbd", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "74a8b6af5097e545", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "343ff2631a3f4b44", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "d49b7701ba9c49e9", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "8ac72ae0ce42207c", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "37f86664cd9263fc", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "2d781b1da70a13c2", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "a9ebd415695ae009", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "d91ca42a4533b4c3", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "9e71709890d7df1d", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "7ce52282790eddaa", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "7793835151b82d90", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "fec514f781e63260", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-13b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-13b-hf", "architecture": "llama", "parameter_count": "13B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "45b133e206ab740a", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "87da984cba6ec6c7", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "5b67586cefbda6f4", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "0cfffe9a1a6f6e12", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "e4cc4c10e5dcac20", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "838ffbe28bbedbd4", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "65de270be987cdea", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "bac09fe36d1461a3", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "bb99a32c21a79460", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "1f0ea4daaf59a54a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "fd2f0598de89aaeb", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "d491ed6b636f0f5d", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "e82a51cc39dd6959", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "92bc6d25cd852f55", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "b4defc5e5e0b6dff", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "196673c98b893be6", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "f2e59fd0bbce2300", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "c1316b8c98a13b73", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "93bfc09fba200a2f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "3e70bf522e7b7065", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "61b5054ddebb8ea8", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "fa2011d88f80fa41", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "33df0d1a9fe1451e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "5f96c126c5e9b625", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "30b0eb4c837cce91", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "e88f7cba54dd8826", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "02108750933061d7", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "7ab6b0fb7aef22be", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "5ee2556ba3650a28", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "5592f02d717b45cd", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/meta-llama/Llama-2-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "587c78381e4ab15e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/michiyasunaga/BioLinkBERT-base", "confidence": "high"}, "task": {"task_type": "medical_qa", "domain": "medical", "supervised_type": "qa"}, "dataset": {"name": "pubmed_qa", "size": 211269, "language": "en"}, "model": {"name": "michiyasunaga/BioLinkBERT-base", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.8460000000000001, "validation_loss": 0.61}, "id": "a86f51f577b9ee91", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base", "confidence": "high"}, "task": {"task_type": "medical_qa", "domain": "medical", "supervised_type": "qa"}, "dataset": {"name": "medqa", "size": 12723, "language": "en"}, "model": {"name": "microsoft/BiomedNLP-PubMedBERT-base", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.8200000000000001, "validation_loss": 0.57}, "id": "eeb8c6fc7c283dbd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "microsoft/Phi-3-mini-4k-instruct", "architecture": "phi", "parameter_count": "3.8B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 4096, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.1, "target_modules": ["qkv_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.42, "validation_loss": 0.85}, "id": "44877593313f2cc5", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/biogpt", "confidence": "high"}, "task": {"task_type": "medical_text_generation", "domain": "biomedical", "supervised_type": "causal_lm"}, "dataset": {"name": "pubmed_abstracts", "size": 150000, "language": "en"}, "model": {"name": "microsoft/biogpt", "architecture": "gpt2", "parameter_count": "347M", "model_type": "decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 2, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 32, "alpha": 64, "dropout": 0.05, "target_modules": ["c_attn"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 15.2, "validation_loss": 2.72}, "id": "4678030cdd71f983", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/codebert-base", "confidence": "high"}, "task": {"task_type": "code_search", "domain": "programming", "supervised_type": "classification"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "python"}, "model": {"name": "microsoft/codebert-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 32, "gradient_accumulation_steps": 1, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "mrr", "metric_value": 0.68, "validation_loss": 0.42}, "id": "f35699e837eeee53", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/deberta-v3-base", "confidence": "high"}, "task": {"task_type": "acceptability_classification", "domain": "linguistics", "supervised_type": "classification"}, "dataset": {"name": "cola", "size": 8551, "language": "en"}, "model": {"name": "microsoft/deberta-v3-base", "architecture": "deberta", "parameter_count": "184M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9329999999999999, "validation_loss": 0.6799999999999999}, "id": "e073fb9f4a6c2b7d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/deberta-v3-base", "confidence": "high"}, "task": {"task_type": "paraphrase_detection", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "mrpc", "size": 3668, "language": "en"}, "model": {"name": "microsoft/deberta-v3-base", "architecture": "deberta", "parameter_count": "184M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.879, "validation_loss": 0.6399999999999999}, "id": "398b899b0bbc61a8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/deberta-v3-base", "confidence": "high"}, "task": {"task_type": "question_classification", "domain": "qa", "supervised_type": "classification"}, "dataset": {"name": "trec", "size": 5452, "language": "en"}, "model": {"name": "microsoft/deberta-v3-base", "architecture": "deberta", "parameter_count": "184M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.907, "validation_loss": 0.42}, "id": "27d53067c456f090", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/deberta-v3-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "microsoft/deberta-v3-base", "architecture": "deberta", "parameter_count": "184M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.871, "validation_loss": 0.5599999999999999}, "id": "df2b70d8020a75a9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/deberta-v3-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "microsoft/deberta-v3-base", "architecture": "deberta", "parameter_count": "184M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9159999999999999, "validation_loss": 0.51}, "id": "5ee06fcb20147a60", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/deberta-v3-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "microsoft/deberta-v3-base", "architecture": "deberta", "parameter_count": "184M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9219999999999999, "validation_loss": 0.57}, "id": "7cb1ffed9705987e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/deberta-v3-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "knowledge", "supervised_type": "classification"}, "dataset": {"name": "dbpedia_14", "size": 560000, "language": "en"}, "model": {"name": "microsoft/deberta-v3-base", "architecture": "deberta", "parameter_count": "184M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9269999999999999, "validation_loss": 0.62}, "id": "a8d729ef70440904", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/deberta-v3-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "microsoft/deberta-v3-base", "architecture": "deberta", "parameter_count": "184M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.884, "validation_loss": 0.69}, "id": "f825b632c8d25448", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "math_reasoning", "domain": "mathematics", "supervised_type": "causal_lm"}, "dataset": {"name": "gsm8k", "size": 7473, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 2, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 32, "alpha": 64, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "dense"], "quantization": "none"}, "hardware": {"gpu_type": "RTX_4090", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.72, "validation_loss": 0.68}, "id": "4ff12c2bde4dc544", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9239999999999999, "validation_loss": 0.52}, "id": "1ba4d48b18c19357", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9239999999999999, "validation_loss": 0.52}, "id": "202b2a239b798bb3", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9239999999999999, "validation_loss": 0.52}, "id": "50fcb410ef4b084c", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9239999999999999, "validation_loss": 0.52}, "id": "0d9a63b0848696cb", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.52}, "id": "9e0fad6e9cdcf4c6", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.52}, "id": "f560ad3480b6df88", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.52}, "id": "ae0d4c2c56bb9b8a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.52}, "id": "0bea92b7999d3c99", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9309999999999999, "validation_loss": 0.52}, "id": "3438dd93fbbe5dce", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9309999999999999, "validation_loss": 0.52}, "id": "7c26ad5aecd637b6", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9309999999999999, "validation_loss": 0.52}, "id": "b8681d8af2365c61", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9309999999999999, "validation_loss": 0.52}, "id": "b74a8b27fc295e87", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.6799999999999999}, "id": "56a36aea93f2b5c7", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.6799999999999999}, "id": "63badf7b78823be9", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.6799999999999999}, "id": "703e06febb4a2d3b", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.6799999999999999}, "id": "823ded859d0950d1", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9159999999999999, "validation_loss": 0.6799999999999999}, "id": "f1c5599b56ea874a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9159999999999999, "validation_loss": 0.6799999999999999}, "id": "fb296e53056ad61a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9159999999999999, "validation_loss": 0.6799999999999999}, "id": "c79762cb6064ff0d", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9159999999999999, "validation_loss": 0.6799999999999999}, "id": "92408f46de253274", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.9199999999999999}, "id": "a5b459a90ff9ee56", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.9199999999999999}, "id": "394d7f2b4d51cce1", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.9199999999999999}, "id": "1df45f7cc6b576ff", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.9199999999999999}, "id": "c75a67a66d8f483e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.9199999999999999}, "id": "7616d7ecdc9e46c1", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.9199999999999999}, "id": "4986142a2c7492e9", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.9199999999999999}, "id": "7c027f045470d558", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.948, "validation_loss": 0.9199999999999999}, "id": "e6d236c440bc118a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.941, "validation_loss": 0.9199999999999999}, "id": "8b43cf369b4eee73", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.941, "validation_loss": 0.9199999999999999}, "id": "4dc32e02229db8df", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.941, "validation_loss": 0.9199999999999999}, "id": "9862c67fc53ed86a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.941, "validation_loss": 0.9199999999999999}, "id": "b4c6a33eeea3d8f4", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.9199999999999999}, "id": "58c753b94bdefd78", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.9199999999999999}, "id": "6eea90f4fe8c7b1b", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.9199999999999999}, "id": "36610cc92089ad60", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.9199999999999999}, "id": "626640ab6c54f1a3", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.938, "validation_loss": 0.9199999999999999}, "id": "3be87cfaa7a6da68", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.938, "validation_loss": 0.9199999999999999}, "id": "794bfbbc17bbcda6", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.938, "validation_loss": 0.9199999999999999}, "id": "8724a6e05e3e430e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/microsoft/phi-2", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "microsoft/phi-2", "architecture": "phi", "parameter_count": "2.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 1024, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.938, "validation_loss": 0.9199999999999999}, "id": "11a26bf71dc49e9c", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "2d1caa50ae457cec", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "d1575d9edddb4d3c", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "332821381bc13e72", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "64a006a4fb3cc9ea", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "2d19b1777aca95da", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "63c6d0671eba1f69", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "40f929d8e98bccfd", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "5c1d9351bb6dfb7d", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "e2b18a546535de56", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "f50c8cf9e88db858", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "2c627593348acb99", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "23168446b634fc2c", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "425c874e4c0fc005", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "03c01525fc198063", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "47ad95689709c816", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "441b107b3676d590", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "c9b2d8eb8d3b4b87", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "7cf002f8c23e7b5c", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "d734b91d91349019", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "0a3a4d430f70fb78", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "5e4923a1e66add7f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "f32bda9bdc4f3494", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "55d3d9beaac4ce65", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "9302b0ab3d453381", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "3650b707a7feac84", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "66d2ea8e3c515b68", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "f7bc6532072b80cb", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "59d7636099be410d", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "d415adc08ea52668", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "607bc8303db50df5", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mistral-7B-v0.1", "confidence": "high"}, "task": {"task_type": "text2sql", "domain": "database", "supervised_type": "seq2seq"}, "dataset": {"name": "b-mc2/sql-create-context", "size": 78577, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-v0.1", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 2, "effective_batch_size": 8, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "exact_match", "metric_value": 0.68, "validation_loss": 0.94}, "id": "92a60b1e0a3c2a19", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/mistralai/Mixtral-8x7B-v0.1", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "ultrachat_200k", "size": 200000, "language": "en"}, "model": {"name": "mistralai/Mixtral-8x7B-v0.1", "architecture": "mixtral", "parameter_count": "46.7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 2.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 32, "effective_batch_size": 32, "optimizer": "paged_adamw_32bit", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.0, "max_seq_length": 4096, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 32, "alpha": 64, "dropout": 0.05, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"], "quantization": "nf4"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 2}, "performance": {"metric_name": "accuracy", "metric_value": 0.86, "validation_loss": 0.78}, "id": "2eb92678d6518cbd", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/nlpaueb/legal-bert-base-uncased", "confidence": "high"}, "task": {"task_type": "legal_classification", "domain": "legal", "supervised_type": "classification"}, "dataset": {"name": "lex_glue", "size": 55000, "language": "en"}, "model": {"name": "nlpaueb/legal-bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 4.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.79, "validation_loss": 0.52}, "id": "b281d859c0f59db5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/nlpaueb/legal-bert-base-uncased", "confidence": "high"}, "task": {"task_type": "legal_classification", "domain": "legal", "supervised_type": "classification"}, "dataset": {"name": "lex_glue", "size": 55000, "language": "en"}, "model": {"name": "nlpaueb/legal-bert-base-uncased", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 4.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.817, "validation_loss": 0.67}, "id": "b281d859c0f59db5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/openai/clip-vit-base-patch32", "confidence": "high"}, "task": {"task_type": "image_classification", "domain": "vision", "supervised_type": "classification"}, "dataset": {"name": "imagenet_1k", "size": 1281167, "language": "en"}, "model": {"name": "openai/clip-vit-base-patch32", "architecture": "clip", "parameter_count": "151M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 10.0, "batch_size_per_device": 64, "gradient_accumulation_steps": 1, "effective_batch_size": 64, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.1, "weight_decay": 0.1, "max_seq_length": null, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 16, "dropout": 0.0, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "top1_accuracy", "metric_value": 0.76, "validation_loss": 0.95}, "id": "1b6735980fc00275", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/pile-of-law/legalbert-large-1.7M-1", "confidence": "high"}, "task": {"task_type": "legal_prediction", "domain": "legal", "supervised_type": "classification"}, "dataset": {"name": "case_outcomes", "size": 25000, "language": "en"}, "model": {"name": "pile-of-law/legalbert-large-1.7M-1", "architecture": "bert", "parameter_count": "340M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 4.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.75, "validation_loss": 0.78}, "id": "a04bb7285d55b66c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "acceptability_classification", "domain": "linguistics", "supervised_type": "classification"}, "dataset": {"name": "cola", "size": 8551, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.889, "validation_loss": 0.74}, "id": "a55e435d8d83fdda", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.66, "validation_loss": 0.52}, "id": "584256bc73cf7736", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.66, "validation_loss": 0.52}, "id": "d2f80cc493973425", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.66, "validation_loss": 0.52}, "id": "353e63cf5efea799", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.66, "validation_loss": 0.52}, "id": "e043512452551992", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "entertainment", "supervised_type": "ner"}, "dataset": {"name": "mit_movie", "size": 9775, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.926, "validation_loss": 0.33999999999999997}, "id": "ef34ec289a0ca953", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.892, "validation_loss": 0.39999999999999997}, "id": "428cddb80adc377b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.892, "validation_loss": 0.39999999999999997}, "id": "5a95f8acf789a433", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.892, "validation_loss": 0.39999999999999997}, "id": "bbd9aa073ab84d95", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.89, "validation_loss": 0.28}, "id": "7e2a996eafa79593", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.892, "validation_loss": 0.39999999999999997}, "id": "6f2551e2746d93f2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.941, "validation_loss": 0.39999999999999997}, "id": "892cd30616458ea8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.941, "validation_loss": 0.39999999999999997}, "id": "d2d39ce011205b24", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.941, "validation_loss": 0.39999999999999997}, "id": "85e8233029069426", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.95, "validation_loss": 0.28}, "id": "67293df2f8087e23", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.941, "validation_loss": 0.39999999999999997}, "id": "32f94e1d9463d3dc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "restaurant", "supervised_type": "ner"}, "dataset": {"name": "mit_restaurant", "size": 7660, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.937, "validation_loss": 0.25}, "id": "69a342faa3811ea1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.938, "validation_loss": 0.39999999999999997}, "id": "8e2c355b88638390", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.938, "validation_loss": 0.39999999999999997}, "id": "59b82c1ed12edd80", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.938, "validation_loss": 0.39999999999999997}, "id": "19d998fda918f46c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9390000000000001, "validation_loss": 0.27}, "id": "d2c12f7e44a349d6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.938, "validation_loss": 0.39999999999999997}, "id": "a64447c26934a32d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "paraphrase_detection", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "mrpc", "size": 3668, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.899, "validation_loss": 0.84}, "id": "c63794626a05ab1b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "paraphrase_detection", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "paws", "size": 49401, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 4.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 2, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 128, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.95, "validation_loss": 0.18}, "id": "2e7b1929d0ef1afd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "conversation", "supervised_type": "qa"}, "dataset": {"name": "quac", "size": 98407, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9249999999999999, "validation_loss": 1.2}, "id": "706145fa31999106", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.907, "validation_loss": 0.45999999999999996}, "id": "5325d2802a7d5bfb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.907, "validation_loss": 0.45999999999999996}, "id": "4c47bba9d85a6343", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9159999999999999, "validation_loss": 1.21}, "id": "84c694d2b99715e1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.907, "validation_loss": 0.45999999999999996}, "id": "99dfcad37cd9b079", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.907, "validation_loss": 0.45999999999999996}, "id": "ba5d6fd876965cca", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.898, "validation_loss": 0.45999999999999996}, "id": "cd8216775f931e88", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.898, "validation_loss": 0.45999999999999996}, "id": "46a4da244bb99554", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9109999999999999, "validation_loss": 0.96}, "id": "68a95fd3a247a3b7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.898, "validation_loss": 0.45999999999999996}, "id": "efc68d7dd83f886d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.898, "validation_loss": 0.45999999999999996}, "id": "74df014b74da84f4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "multi_hop", "supervised_type": "qa"}, "dataset": {"name": "hotpot_qa", "size": 90447, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.844, "validation_loss": 0.99}, "id": "57b8914091f5000e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9229999999999999, "validation_loss": 0.45999999999999996}, "id": "2ac3d2f6fd340343", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9229999999999999, "validation_loss": 0.45999999999999996}, "id": "c152befeb1a32862", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.858, "validation_loss": 1.03}, "id": "9fd5ed53fad1086c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9229999999999999, "validation_loss": 0.45999999999999996}, "id": "b773bbd20a5d3c4b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9229999999999999, "validation_loss": 0.45999999999999996}, "id": "489bd259f243dc4e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "question_classification", "domain": "qa", "supervised_type": "classification"}, "dataset": {"name": "trec", "size": 5452, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.886, "validation_loss": 0.71}, "id": "df330c1a77d195b4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.909, "validation_loss": 0.6599999999999999}, "id": "4124d0edad5b320b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.909, "validation_loss": 0.6599999999999999}, "id": "14ae276b3a1aee3d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.909, "validation_loss": 0.6599999999999999}, "id": "f563a375bc957f3a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.909, "validation_loss": 0.6599999999999999}, "id": "2725e283a02937e1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.908, "validation_loss": 0.43}, "id": "df85793ec5771206", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.6599999999999999}, "id": "6d7a67b5f4e05ce3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.6599999999999999}, "id": "9d4fcaac3f0d3ddb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.86, "validation_loss": 0.44999999999999996}, "id": "2c50fc3dfa368cf9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.6599999999999999}, "id": "baf6bee2bdbdd062", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9299999999999999, "validation_loss": 0.6599999999999999}, "id": "b7eae8e080872a4f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.919, "validation_loss": 0.54}, "id": "61a970330776dff7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.855, "validation_loss": 0.94}, "id": "e150fdc4a86407dd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.855, "validation_loss": 0.94}, "id": "81857b4b97638459", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.855, "validation_loss": 0.94}, "id": "e84061d4393ae64b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.855, "validation_loss": 0.94}, "id": "2d57ad44bc3b2f0a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.94}, "id": "af892826a071410b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.94}, "id": "9e7ed03e2d233ddb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.94}, "id": "604322dfab8fe50f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.86, "validation_loss": 0.94}, "id": "49ad183a647af155", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "knowledge", "supervised_type": "classification"}, "dataset": {"name": "dbpedia_14", "size": 560000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9, "validation_loss": 0.35}, "id": "47b16389d0456bd4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9309999999999999, "validation_loss": 0.79}, "id": "d8485660e6040221", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9309999999999999, "validation_loss": 0.79}, "id": "64b651266e6593f6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.913, "validation_loss": 0.48}, "id": "bbfc996147cadda7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9309999999999999, "validation_loss": 0.79}, "id": "e3575453268325f6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9309999999999999, "validation_loss": 0.79}, "id": "54579c48f743944f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.79}, "id": "cd8bd8adaa685212", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.79}, "id": "2b11e40a7bd9c049", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.79}, "id": "9fa214626aaee4ff", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.94, "validation_loss": 0.79}, "id": "2ed67db941761f61", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.79}, "id": "362787c7d71415ed", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.79}, "id": "5fc276501eaf3947", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.79}, "id": "7667abfca0dc6550", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9289999999999999, "validation_loss": 0.79}, "id": "0760ff117f09d820", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.859, "validation_loss": 0.6399999999999999}, "id": "00e2922db019b20c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.859, "validation_loss": 0.6399999999999999}, "id": "5486832920d7cf8a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.859, "validation_loss": 0.6399999999999999}, "id": "13ede4c2a97d9a5e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "roberta-base", "architecture": "roberta", "parameter_count": "125M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.859, "validation_loss": 0.6399999999999999}, "id": "c0bcc8b75db482e2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "conversation", "supervised_type": "qa"}, "dataset": {"name": "quac", "size": 98407, "language": "en"}, "model": {"name": "roberta-large", "architecture": "roberta", "parameter_count": "355M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.887, "validation_loss": 1.12}, "id": "6a86715bcdb2d35f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "roberta-large", "architecture": "roberta", "parameter_count": "355M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.8909999999999999, "validation_loss": 1.26}, "id": "2b72a57b1e6ad0bc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "roberta-large", "architecture": "roberta", "parameter_count": "355M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.823, "validation_loss": 1.28}, "id": "2654ded080df1152", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "multi_hop", "supervised_type": "qa"}, "dataset": {"name": "hotpot_qa", "size": 90447, "language": "en"}, "model": {"name": "roberta-large", "architecture": "roberta", "parameter_count": "355M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9219999999999999, "validation_loss": 1.27}, "id": "4208437bee1d19a3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "roberta-large", "architecture": "roberta", "parameter_count": "355M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 12, "gradient_accumulation_steps": 2, "effective_batch_size": 24, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 384, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.879, "validation_loss": 1.04}, "id": "2e3eb4059ff9739f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/starcoder", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "apps", "size": 10000, "language": "python"}, "model": {"name": "starcoder", "architecture": "starcoder", "parameter_count": "15.5B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "pass@1", "metric_value": 0.44799999999999995, "validation_loss": 0.78}, "id": "dd05edb5b6ed1ec4", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/starcoder", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "python"}, "model": {"name": "starcoder", "architecture": "starcoder", "parameter_count": "15.5B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "pass@1", "metric_value": 0.494, "validation_loss": 0.84}, "id": "81b92d7b810d2b36", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/starcoder", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "codeparrot/github-code", "size": 115000000, "language": "python"}, "model": {"name": "starcoder", "architecture": "starcoder", "parameter_count": "15.5B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 1.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "pass@1", "metric_value": 0.39299999999999996, "validation_loss": 0.63}, "id": "057d7e79802d0eac", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/starcoder", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "humaneval", "size": 164, "language": "python"}, "model": {"name": "starcoder", "architecture": "starcoder", "parameter_count": "15.5B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "pass@1", "metric_value": 0.502, "validation_loss": 0.9199999999999999}, "id": "61b190be1074463d", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/starcoder", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "python"}, "model": {"name": "starcoder", "architecture": "starcoder", "parameter_count": "15.5B", "model_type": "decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.05, "weight_decay": 0.1, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 4}, "performance": {"metric_name": "pass@1", "metric_value": 0.40299999999999997, "validation_loss": 0.73}, "id": "1e3e597c3ba45947", "derived": {"model_size_bucket": "large", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.52, "validation_loss": 0.77}, "id": "08ac74a96817c409", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.52, "validation_loss": 0.77}, "id": "a87ea989c8dc4192", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.52, "validation_loss": 0.77}, "id": "9272b2b4f5bb6b48", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.52, "validation_loss": 0.77}, "id": "caca9745a924d999", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.876, "validation_loss": 0.9199999999999999}, "id": "d12d4bd7daf4e7fb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.876, "validation_loss": 0.9199999999999999}, "id": "23d0c5a9fb5e91ef", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.876, "validation_loss": 0.9199999999999999}, "id": "ee3b8d10c593a43c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.876, "validation_loss": 0.9199999999999999}, "id": "032e623bdebee057", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.885, "validation_loss": 0.9199999999999999}, "id": "ed59841301c85f0a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.885, "validation_loss": 0.9199999999999999}, "id": "e8a8a07eac819e77", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.885, "validation_loss": 0.9199999999999999}, "id": "eca86f1d496b9d55", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.885, "validation_loss": 0.9199999999999999}, "id": "2dbc3f1373c63c2d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9239999999999999, "validation_loss": 0.9199999999999999}, "id": "414a6090ee65ce56", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9239999999999999, "validation_loss": 0.9199999999999999}, "id": "76754ad3ead8eb2b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9239999999999999, "validation_loss": 0.9199999999999999}, "id": "320b2fcd31974b1d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9239999999999999, "validation_loss": 0.9199999999999999}, "id": "af5b2de6e1828600", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "paraphrase", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "paws", "size": 49401, "language": "multilingual"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.383, "validation_loss": 1.53}, "id": "64d9cc75ef21614d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.855, "validation_loss": 0.35}, "id": "6ae42ff473b9c304", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.855, "validation_loss": 0.35}, "id": "7e836576e0dd2918", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.855, "validation_loss": 0.35}, "id": "445232e5db54eae8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.855, "validation_loss": 0.35}, "id": "c1321cd729cf2bc7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.901, "validation_loss": 0.35}, "id": "ffad06b8ebbbd96f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.901, "validation_loss": 0.35}, "id": "d5395cee4704bfa4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.901, "validation_loss": 0.35}, "id": "a440e1a9a777fb2d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.901, "validation_loss": 0.35}, "id": "577f77b1ca139704", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.945, "validation_loss": 0.35}, "id": "6620df9c69f6aefb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.945, "validation_loss": 0.35}, "id": "8ad0bcd5a5c44201", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.945, "validation_loss": 0.35}, "id": "39a4f2d851abc462", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.945, "validation_loss": 0.35}, "id": "bd5bf06a95924008", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.889, "validation_loss": 0.72}, "id": "3ff9813b5f5302d7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.889, "validation_loss": 0.72}, "id": "8f382845b90ea04a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.889, "validation_loss": 0.72}, "id": "5f8e2d40f35830ce", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.889, "validation_loss": 0.72}, "id": "0f692cfab128d419", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.877, "validation_loss": 0.72}, "id": "eebd08c239ac36d5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.877, "validation_loss": 0.72}, "id": "0100d8eb382b1807", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.877, "validation_loss": 0.72}, "id": "a4a94bbd88185ff7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.877, "validation_loss": 0.72}, "id": "8a46179412d2f0f4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "conversation", "supervised_type": "seq2seq"}, "dataset": {"name": "samsum", "size": 14732, "language": "multilingual"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.45299999999999996, "validation_loss": 1.33}, "id": "8e3ce2fa9c0976fc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.948, "validation_loss": 0.86}, "id": "3d47cbffb0d4436c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.948, "validation_loss": 0.86}, "id": "ad6f81dcb6c6ceb5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.948, "validation_loss": 0.86}, "id": "31393aad90f7c007", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.948, "validation_loss": 0.86}, "id": "fdf72047aef11d88", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "multilingual"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.473, "validation_loss": 1.53}, "id": "fdf72047aef11d88", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.943, "validation_loss": 0.86}, "id": "e125d79b664188b1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.943, "validation_loss": 0.86}, "id": "61e6a0a5932e4067", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.943, "validation_loss": 0.86}, "id": "b450211d5fbc5de7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_2", "metric_value": 0.22, "validation_loss": 1.45}, "id": "190012d12d99d044", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.943, "validation_loss": 0.86}, "id": "190012d12d99d044", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "multilingual"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.44799999999999995, "validation_loss": 1.28}, "id": "190012d12d99d044", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.945, "validation_loss": 0.42}, "id": "84d449c9d819a35d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.945, "validation_loss": 0.42}, "id": "ffc16cbf2507c0ef", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.945, "validation_loss": 0.42}, "id": "f776e7df21fefcde", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.945, "validation_loss": 0.42}, "id": "55a597881a7957dc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9119999999999999, "validation_loss": 0.42}, "id": "8f4a77d95a156124", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9119999999999999, "validation_loss": 0.42}, "id": "138077e7603ba54e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9119999999999999, "validation_loss": 0.42}, "id": "b0d66ff752916a8c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9119999999999999, "validation_loss": 0.42}, "id": "f8f42ad964198ef2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.887, "validation_loss": 0.42}, "id": "cb8d0ddc57f71c41", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.887, "validation_loss": 0.42}, "id": "6fa9965bbb372d6d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.887, "validation_loss": 0.42}, "id": "e20b6ace15b9f8d6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.887, "validation_loss": 0.42}, "id": "ee41ff8dc57e6e01", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.863, "validation_loss": 0.91}, "id": "d569d84140d6b0e4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.863, "validation_loss": 0.91}, "id": "c8c61215bcf9c743", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.863, "validation_loss": 0.91}, "id": "caf06c55874a4f46", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.418, "validation_loss": 1.28}, "id": "c035ac7379d1c96b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.863, "validation_loss": 0.91}, "id": "c035ac7379d1c96b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-base", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt16_en_ro", "size": 610000, "language": "en"}, "model": {"name": "t5-base", "architecture": "t5", "parameter_count": "220M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.434, "validation_loss": 1.44}, "id": "8aa4fb90b9a46685", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.6499999999999999}, "id": "f7c38f08ebaea5b7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.6499999999999999}, "id": "d1492df945dd87b2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.6499999999999999}, "id": "b76159f1bb1986a3", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.6499999999999999}, "id": "42a9be5846b21add", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.6499999999999999}, "id": "7475b65c0f7485ac", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.6499999999999999}, "id": "ef74966fe5839f98", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.6499999999999999}, "id": "4898d8517a8aad88", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.6499999999999999}, "id": "eca1bf2ecbb42f9a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "e1b2910122649737", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "6c7cd8d6bd071d75", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "1b2e3f6ef21815fc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "ca44e81f60d1c9d4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "4eddd481374e88cf", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "cff29c71398d24e8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "65b6e2027285b90d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "2d5c7c70ca215e93", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "1f104902df74c887", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "c6b8f7610ec74e3d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "d1f8e564187532f8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "185b9d11d8504efa", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "548ba069584a2990", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "608459f3d49ef347", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "aabeba5329f1ab08", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.57}, "id": "6954028df3249d1e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "4d5e23eef3a1d3a0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "d6b813e95b59de33", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "7c643befe856e564", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "bb478def554c80e6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "e7a573164a4851b8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "201159dd262a1682", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "ad6f3153eedf5591", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "8ca64c78c5f3590b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "ec8f2148fedc7a24", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "28dce152269a2a60", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "2c2492d780b5b2cb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "bc0693e9611ea42e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "cfcc44e126951f0b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "412a0e4ca47904fa", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "87d0131a3274c59b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.3100000000000005, "validation_loss": 0.88}, "id": "b7734870c0e3625f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "paraphrase", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "paws", "size": 49401, "language": "multilingual"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.38599999999999995, "validation_loss": 1.56}, "id": "2191af850be930be", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.72}, "id": "5e3da95c36d9d2c6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.72}, "id": "7d23bfdbbe8e8324", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.72}, "id": "17fd38b0db83fff6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.72}, "id": "7b675ace6405fd15", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.72}, "id": "d6c83f20372a13d9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.72}, "id": "4b5dd049d08b1c55", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.72}, "id": "28afa3712636d1ea", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.72}, "id": "cdbe6307c0aa05d0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "conversation", "supervised_type": "seq2seq"}, "dataset": {"name": "samsum", "size": 14732, "language": "multilingual"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.44199999999999995, "validation_loss": 1.52}, "id": "3e25d96ac0886efe", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.919, "validation_loss": 0.36}, "id": "edba60137dfc266e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.919, "validation_loss": 0.36}, "id": "9db82b655daae2f2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.919, "validation_loss": 0.36}, "id": "e69f4b57b11f5601", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.919, "validation_loss": 0.36}, "id": "f533b6e10f22ac41", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.919, "validation_loss": 0.36}, "id": "8f37ccf11e467bf4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.919, "validation_loss": 0.36}, "id": "abdffc76ab00e57b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.919, "validation_loss": 0.36}, "id": "8a93bf8a6c7350f2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.919, "validation_loss": 0.36}, "id": "2f0b1f419c900bb7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "multilingual"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.496, "validation_loss": 1.76}, "id": "8a93bf8a6c7350f2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.36}, "id": "883f2b0d7f11c844", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.36}, "id": "6d4cc498748b30b0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.36}, "id": "50b4bb5c13668420", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.36}, "id": "bedf1a008dcf775e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.36}, "id": "808e9a29866be7c9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.36}, "id": "23f30636f436457e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.36}, "id": "c451ecc3a0182a9a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.886, "validation_loss": 0.36}, "id": "d9c58d8b99a3ec4d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "multilingual"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.44399999999999995, "validation_loss": 1.54}, "id": "c451ecc3a0182a9a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.43999999999999995}, "id": "ce0de0a6e96fb840", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.43999999999999995}, "id": "f75385edeb38659e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.43999999999999995}, "id": "46918df63e9bcc8d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.43999999999999995}, "id": "4510550b17367f9e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.43999999999999995}, "id": "d6424bf578c2c7fd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.43999999999999995}, "id": "dc39c64b91e3af4d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.43999999999999995}, "id": "4a6adb7060bc974b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.867, "validation_loss": 0.43999999999999995}, "id": "b97d30ccf318b4cd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.492, "validation_loss": 1.42}, "id": "4a6adb7060bc974b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt16_en_ro", "size": 610000, "language": "en"}, "model": {"name": "t5-large", "architecture": "t5", "parameter_count": "770M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 1024, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.413, "validation_loss": 1.53}, "id": "d59b42687177413c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.866, "validation_loss": 0.59}, "id": "db095cd7cb2456c8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.866, "validation_loss": 0.59}, "id": "0cca26ff512812e2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.866, "validation_loss": 0.59}, "id": "6abb72ae6c17ec44", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.866, "validation_loss": 0.59}, "id": "f978d70737ff380c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.59}, "id": "d8569053e4ee1a91", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.59}, "id": "4ea0e4fadf71ea87", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.59}, "id": "13d6b3aa24a21d4b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.883, "validation_loss": 0.59}, "id": "39e87e0fa3af18d5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.876, "validation_loss": 0.59}, "id": "17717563ad82fad5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.876, "validation_loss": 0.59}, "id": "2031dce8a9635b16", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.876, "validation_loss": 0.59}, "id": "f043c19fcf41e83e", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.876, "validation_loss": 0.59}, "id": "4124cdc2859efdf4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "paraphrase", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "paws", "size": 49401, "language": "multilingual"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.371, "validation_loss": 1.71}, "id": "fbf8d6dc65e23e16", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.889, "validation_loss": 0.6699999999999999}, "id": "38d617b4a1e20da7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.889, "validation_loss": 0.6699999999999999}, "id": "bbdd8b2137bd563c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.889, "validation_loss": 0.6699999999999999}, "id": "39dd919e82e2a4dc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad", "size": 87599, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.889, "validation_loss": 0.6699999999999999}, "id": "0b1483fda58fbc0d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.88, "validation_loss": 0.6699999999999999}, "id": "628e3364df5ea509", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.88, "validation_loss": 0.6699999999999999}, "id": "239f2508deb87095", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.88, "validation_loss": 0.6699999999999999}, "id": "94b8e5b36d2b1fb0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "general", "supervised_type": "qa"}, "dataset": {"name": "squad_v2", "size": 130319, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.88, "validation_loss": 0.6699999999999999}, "id": "4644174efe865710", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.88, "validation_loss": 0.75}, "id": "afc167a1161b32b2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.88, "validation_loss": 0.75}, "id": "c6b8de2d6026482a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.88, "validation_loss": 0.75}, "id": "891b31bcda7fd758", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "finance", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial", "size": 11932, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.88, "validation_loss": 0.75}, "id": "43e19cc3fe228f37", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.911, "validation_loss": 0.75}, "id": "802edb4fb39020ff", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.911, "validation_loss": 0.75}, "id": "bf47d44ec8eca04a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.911, "validation_loss": 0.75}, "id": "209c436438974eb6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "sst2", "size": 67349, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.911, "validation_loss": 0.75}, "id": "60194f8079de9beb", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "conversation", "supervised_type": "seq2seq"}, "dataset": {"name": "samsum", "size": 14732, "language": "multilingual"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.40399999999999997, "validation_loss": 1.74}, "id": "90160347b87d5248", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "multilingual"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.367, "validation_loss": 1.3699999999999999}, "id": "3f1fad966df0ab9d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "multilingual"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "rouge_l", "metric_value": 0.485, "validation_loss": 1.3499999999999999}, "id": "eff21f937a74c506", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.6799999999999999}, "id": "2e05aa077d4b1b91", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.6799999999999999}, "id": "a37b7956aecd2bb9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.6799999999999999}, "id": "07fed3bd43fbae9d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "news", "supervised_type": "classification"}, "dataset": {"name": "ag_news", "size": 120000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9359999999999999, "validation_loss": 0.6799999999999999}, "id": "6f1702999e0f75e1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.857, "validation_loss": 0.6799999999999999}, "id": "9ef5adbab1e33bef", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.857, "validation_loss": 0.6799999999999999}, "id": "02cd0f877779bb45", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.857, "validation_loss": 0.6799999999999999}, "id": "f9a758e050475050", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.857, "validation_loss": 0.6799999999999999}, "id": "2c8a1373bf16b22c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.6799999999999999}, "id": "472e7909a2e21ef4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.6799999999999999}, "id": "f827013ddfc2ce29", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.6799999999999999}, "id": "00240b2620706817", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "text_classification", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "yelp_polarity", "size": 560000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 4, "alpha": 8, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.893, "validation_loss": 0.6799999999999999}, "id": "5734bd59da1b3df9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.43, "validation_loss": 1.4}, "id": "97e09bab0abe3baf", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt16_de_en", "size": 4500000, "language": "de-en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 2, "effective_batch_size": 32, "optimizer": "adafactor", "scheduler": "constant", "warmup_ratio": 0.0, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 28.4, "validation_loss": 1.82}, "id": "78752e390811e3ff", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/t5-small", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt16_en_ro", "size": 610000, "language": "en"}, "model": {"name": "t5-small", "architecture": "t5", "parameter_count": "60M", "model_type": "encoder-decoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 4, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.05, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 32, "dropout": 0.1, "target_modules": ["q", "v"], "quantization": "none"}, "hardware": {"gpu_type": "A10G", "gpu_memory_gb": 24, "num_gpus": 1}, "performance": {"metric_name": "bleu", "metric_value": 0.40199999999999997, "validation_loss": 1.72}, "id": "724167f2eb4345c8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "open_assistant", "size": 84437, "language": "multilingual"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "paged_adamw_32bit", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 64, "alpha": 16, "dropout": 0.1, "target_modules": ["query_key_value", "dense"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.2, "validation_loss": 1.43}, "id": "8ec6c88d1b7e9ca0", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "cc6ef790fd01081f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "39464bc6a01893d2", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "ab35c93b9753d2aa", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "5e4a62c901898731", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "cf25d0e178136003", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "e5a0046aee7b528a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "d2abe3a0f4ee2369", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "910f34212d0a4cf5", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "5fa01c4c5f55d6c4", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "23981ac095adb7d2", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "a87e27f2c4b9ae28", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "de3cf8877169a79d", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "4bcea306fe18f3af", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "3358e5ad84f394d5", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "56d2a4265df4a225", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "75b7f4221d8e08c5", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "901962b4b67cfcb5", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "384928bcef1917a7", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "25b02555795ff3b3", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "8ecca5c942d67fae", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "b3b12b1bb274b32e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "8a717a7739e5404f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "17253f14e438a3d9", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "e934bcd13ada6c6a", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "98739a7d87b1b1d8", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0001, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "9a602bd363bc8a7b", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "fef3f322a9a771df", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "a3056c977ee8df5f", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 1, "gradient_accumulation_steps": 16, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "qlora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "4bit"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.4800000000000004, "validation_loss": 1.2400000000000002}, "id": "456c585bee06a0f8", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/tiiuae/falcon-7b", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "tiiuae/falcon-7b", "architecture": "falcon", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 3.0, "batch_size_per_device": 2, "gradient_accumulation_steps": 8, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2800000000000002, "validation_loss": 1.1400000000000001}, "id": "df041dbb1f7c73de", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "entertainment", "supervised_type": "ner"}, "dataset": {"name": "mit_movie", "size": 9775, "language": "en"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.927, "validation_loss": 0.35}, "id": "038bc45c978e93b7", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "general", "supervised_type": "ner"}, "dataset": {"name": "ontonotes", "size": 59924, "language": "en"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.9390000000000001, "validation_loss": 0.47}, "id": "407e44a394269535", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "news", "supervised_type": "ner"}, "dataset": {"name": "conll2003", "size": 14041, "language": "en"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.952, "validation_loss": 0.19999999999999998}, "id": "6469cf252e040cc1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "restaurant", "supervised_type": "ner"}, "dataset": {"name": "mit_restaurant", "size": 7660, "language": "en"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.898, "validation_loss": 0.36}, "id": "1a412490fda8ff7b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "high"}, "task": {"task_type": "named_entity_recognition", "domain": "social_media", "supervised_type": "ner"}, "dataset": {"name": "wnut_17", "size": 3394, "language": "en"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.888, "validation_loss": 0.46}, "id": "acdc58e369804362", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "high"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "amazon_reviews_multi", "size": 210000, "language": "multilingual"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.88, "validation_loss": 0.32}, "id": "0d14589ca55342d8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-de", "size": 105215, "language": "en-de"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.85, "validation_loss": 0.77}, "id": "54269e3b888044af", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-es", "size": 56174, "language": "en-es"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.782, "validation_loss": 0.77}, "id": "8f7668736104434c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-fr", "size": 131332, "language": "en-fr"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.76, "validation_loss": 0.77}, "id": "2c791a200997f2a8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-de", "size": 116083, "language": "en-de"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.85, "validation_loss": 0.76}, "id": "7bbf49cff98a4796", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-es", "size": 71913, "language": "en-es"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.782, "validation_loss": 0.76}, "id": "a9ce33f93f5c0490", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-base", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-fr", "size": 114984, "language": "en-fr"}, "model": {"name": "xlm-roberta-base", "architecture": "xlm-roberta", "parameter_count": "270M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.76, "validation_loss": 0.76}, "id": "3e04a843b70801ed", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.69}, "id": "cf69bbf49ed1ba10", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.69}, "id": "cb74e65ea2c9143b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.69}, "id": "fa2f27bad024d7aa", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.69}, "id": "df43ec51344cdcf1", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.69}, "id": "1e8909f77f55de39", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.69}, "id": "095f860657a8aa75", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.69}, "id": "999720eff8d5fd1b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "chat", "domain": "conversation", "supervised_type": "causal_lm"}, "dataset": {"name": "openassistant", "size": 9846, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.69}, "id": "400c7b77dfade017", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "86901c57dfd12696", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "181ea6d10e6dfa8b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "139495b637013f38", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "df3dc750ee63947c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "eb305883f84df4e0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "0c01872bdec6d574", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "26cb0a982b28ec0a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "code_search_net", "size": 2326976, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "c421a62bcb672523", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "2d065242b518f210", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "49f05a2f229048b4", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "32365c7bcefe0643", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "de5b59a12734ed31", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "70af2a337ce540be", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "b691d16999ddd9f6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "e371a9101543f5c2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "code_generation", "domain": "programming", "supervised_type": "causal_lm"}, "dataset": {"name": "mbpp", "size": 974, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.6499999999999999}, "id": "5c52e87f2c110d05", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "4fb6c29375e7ce71", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "337478b6e0cb3540", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "078266bcfcd771fe", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "419e1522814b334c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "17960096d8ed056c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "0c21d8de1a83307d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "528f5398273addd5", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "3e4a128f22c7dffd", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "7dca3db1491efacc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "068858c7636fda45", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "f15b00f92319d324", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "9e6a128572c67cd0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "cbfe92173fba32f2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "5195be63ff7e5478", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "4c25d30c6dc08157", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "dolly", "size": 15015, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 4.08, "validation_loss": 0.87}, "id": "1c7d41d8a3f8e0c0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "light"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.6599999999999999}, "id": "938c8fcb47c166a8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.6599999999999999}, "id": "7ca1a940aa4ba891", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.6599999999999999}, "id": "d2278faa7e68c979", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.6599999999999999}, "id": "718db9d6ab8f2680", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.6599999999999999}, "id": "052ed2c1a417e62b", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.6599999999999999}, "id": "24f03851e1ca43f0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.6599999999999999}, "id": "499e11750330e9a6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "question_answering", "domain": "news", "supervised_type": "qa"}, "dataset": {"name": "newsqa", "size": 92549, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "f1", "metric_value": 0.854, "validation_loss": 0.6599999999999999}, "id": "13b5a54cec5b33ba", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.88, "validation_loss": 0.76}, "id": "f461d2ce55de96d8", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.88, "validation_loss": 0.76}, "id": "5774780bffb04807", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.88, "validation_loss": 0.76}, "id": "49975c568512e874", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.88, "validation_loss": 0.76}, "id": "72e28e345f4610f6", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.88, "validation_loss": 0.76}, "id": "b3f53f5f002ecd3d", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.88, "validation_loss": 0.76}, "id": "1d8e9889b1a120f0", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.88, "validation_loss": 0.76}, "id": "96b460b8b8b16149", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "cnn_dailymail", "size": 287113, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.88, "validation_loss": 0.76}, "id": "16085d2b91651f5a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.941, "validation_loss": 0.76}, "id": "1f5f7f128a77f677", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.941, "validation_loss": 0.76}, "id": "8757d024e07ec426", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.941, "validation_loss": 0.76}, "id": "37b5dc9bb3c3875c", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.941, "validation_loss": 0.76}, "id": "54cdc784037a6142", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.941, "validation_loss": 0.76}, "id": "464c6457667223fa", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.941, "validation_loss": 0.76}, "id": "f6ee4717bab53e4f", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.941, "validation_loss": 0.76}, "id": "9f4515f5fec80cc9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "summarization", "domain": "news", "supervised_type": "seq2seq"}, "dataset": {"name": "xsum", "size": 204045, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.941, "validation_loss": 0.76}, "id": "cd328d6a58f5b210", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9319999999999999, "validation_loss": 0.55}, "id": "2dc1f4610bfd57e9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 1e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9319999999999999, "validation_loss": 0.55}, "id": "abf243789673f1df", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9319999999999999, "validation_loss": 0.55}, "id": "63ab9e25fdf29458", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9319999999999999, "validation_loss": 0.55}, "id": "b0eb6b623120ab38", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9319999999999999, "validation_loss": 0.55}, "id": "a0c5e131f40a0f2a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 3e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9319999999999999, "validation_loss": 0.55}, "id": "12ed4d2b92c5cd47", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9319999999999999, "validation_loss": 0.55}, "id": "28a84227ef76bff2", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "high"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "wmt14_de_en", "size": 4500000, "language": "en"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 5e-05, "epochs": 2.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 8, "effective_batch_size": 32, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp32"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 80, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 0.9319999999999999, "validation_loss": 0.55}, "id": "52e4e31398aa9990", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-de", "size": 66624, "language": "en-de"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.827, "validation_loss": 0.47}, "id": "ef4a8062c966b7ce", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-es", "size": 125022, "language": "en-es"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.884, "validation_loss": 0.47}, "id": "cc7f8fd262163eee", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "general", "supervised_type": "classification"}, "dataset": {"name": "multilingual_sentiment_analysis_en-fr", "size": 147623, "language": "en-fr"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.885, "validation_loss": 0.47}, "id": "fff4586fc6103f72", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-de", "size": 50239, "language": "en-de"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.827, "validation_loss": 0.8}, "id": "befceaac0db02d4a", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-es", "size": 112782, "language": "en-es"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.884, "validation_loss": 0.8}, "id": "9708fb35843334a9", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/xlm-roberta-large", "confidence": "medium"}, "task": {"task_type": "translation", "domain": "general", "supervised_type": "seq2seq"}, "dataset": {"name": "multilingual_translation_en-fr", "size": 124524, "language": "en-fr"}, "model": {"name": "xlm-roberta-large", "architecture": "xlm-roberta", "parameter_count": "550M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.06, "weight_decay": 0.01, "max_seq_length": 256, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 8, "alpha": 16, "dropout": 0.1, "target_modules": ["query", "value"], "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 32, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.885, "validation_loss": 0.8}, "id": "bd402d503b0de9cc", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "large", "training_intensity": "intensive"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/yahma/llama-7b-hf", "confidence": "high"}, "task": {"task_type": "instruction_following", "domain": "general", "supervised_type": "causal_lm"}, "dataset": {"name": "alpaca", "size": 52000, "language": "en"}, "model": {"name": "meta-llama/Llama-2-7b-hf", "architecture": "llama", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0002, "epochs": 3.0, "batch_size_per_device": 4, "gradient_accumulation_steps": 4, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "cosine", "warmup_ratio": 0.03, "weight_decay": 0.0, "max_seq_length": 2048, "precision": "bf16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.05, "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "A100", "gpu_memory_gb": 40, "num_gpus": 1}, "performance": {"metric_name": "perplexity", "metric_value": 3.2, "validation_loss": 1.16}, "id": "33df0d1a9fe1451e", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "huggingface", "url": "https://huggingface.co/yiyanghkust/finbert-tone", "confidence": "high"}, "task": {"task_type": "financial_qa", "domain": "finance", "supervised_type": "qa"}, "dataset": {"name": "fiqa", "size": 6648, "language": "en"}, "model": {"name": "yiyanghkust/finbert-tone", "architecture": "bert", "parameter_count": "110M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 3.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.885, "validation_loss": 0.52}, "id": "3742fc1c2e74a758", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "small", "training_intensity": "moderate"}}
{"source": {"platform": "kaggle", "url": "https://kaggle.com/example/epochs-1", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "reviews", "supervised_type": "classification"}, "dataset": {"name": "imdb", "size": 25000, "language": "en"}, "model": {"name": "distilbert-base-uncased", "architecture": "distilbert", "parameter_count": "66M", "model_type": "encoder"}, "training_config": {"learning_rate": 2e-05, "epochs": 1.0, "batch_size_per_device": 16, "gradient_accumulation_steps": 1, "effective_batch_size": 16, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "none", "r": null, "alpha": null, "dropout": null, "target_modules": null, "quantization": "none"}, "hardware": {"gpu_type": "V100", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "accuracy", "metric_value": 0.9, "validation_loss": 0.31}, "id": "c1a58ce86b2b4637", "derived": {"model_size_bucket": "small", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
{"source": {"platform": "kaggle", "url": "https://www.kaggle.com/code/mistral-sentiment-analysis", "confidence": "medium"}, "task": {"task_type": "sentiment_analysis", "domain": "social_media", "supervised_type": "classification"}, "dataset": {"name": "twitter_financial_news", "size": 11932, "language": "en"}, "model": {"name": "mistralai/Mistral-7B-Instruct-v0.2", "architecture": "mistral", "parameter_count": "7B", "model_type": "decoder"}, "training_config": {"learning_rate": 0.0003, "epochs": 5.0, "batch_size_per_device": 8, "gradient_accumulation_steps": 1, "effective_batch_size": 8, "optimizer": "adamw_torch", "scheduler": "linear", "warmup_ratio": 0.1, "weight_decay": 0.01, "max_seq_length": 512, "precision": "fp16"}, "adapter_config": {"method": "lora", "r": 16, "alpha": 32, "dropout": 0.1, "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"], "quantization": "none"}, "hardware": {"gpu_type": "T4", "gpu_memory_gb": 16, "num_gpus": 1}, "performance": {"metric_name": "f1_macro", "metric_value": 0.84, "validation_loss": 0.42}, "id": "9a46c44613064cdc", "derived": {"model_size_bucket": "medium", "dataset_size_bucket": "medium", "training_intensity": "moderate"}}
